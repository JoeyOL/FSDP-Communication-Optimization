# 基于 PyTorch-FSDP 分布式训练的通信数据压缩研究（开题报告）

哈尔滨工业大学深圳校区

毕业论文开题报告

| 项目 | 内容 |
| --- | --- |
| 题目 | 基于 PyTorch-FSDP 分布式训练的通信数据压缩研究 |
| 姓名 | 王云龙 |
| 学号 | 220110608 |
| 学院 | 计算机科学与技术 |
| 专业 | 计算机科学与技术 |
| 指导教师 | 施少怀 |
| 日期 | 2025年10月3日 |

## 目录

- [1 课题背景及研究的目的和意义](#1-课题背景及研究的目的和意义)
   - [1.1 课题背景](#11-课题背景)
   - [1.2 研究的目的和意义](#12-研究的目的和意义)
- [2 国内外研究现状及分析](#2-国内外研究现状及分析)
   - [2.1 国内外研究现状](#21-国内外研究现状)
   - [2.2 国内外文献综述及简析](#22-国内外文献综述及简析)
      - [2.2.1 分布式训练框架与并行策略研究现状](#221-分布式训练框架与并行策略研究现状)
      - [2.2.2 分布式训练通信优化研究现状](#222-分布式训练通信优化研究现状)
- [3 主要研究内容及研究方案](#3-主要研究内容及研究方案)
   - [3.1 研究内容](#31-研究内容)
      - [3.1.1 训练过程通信瓶颈分析](#311-训练过程通信瓶颈分析)
      - [3.1.2 压缩算法设计与优化](#312-压缩算法设计与优化)
      - [3.1.3 收敛性测试和性能测试](#313-收敛性测试和性能测试)
   - [3.2 研究方案](#32-研究方案)
   - [3.3 与本仓库实现的对齐（复现说明）](#33-与本仓库实现的对齐复现说明)
- [4 进度安排及预期目标](#4-进度安排及预期目标)
   - [4.1 进度安排](#41-进度安排)
   - [4.2 预期目标](#42-预期目标)
- [5 已具备和所需的条件和经费](#5-已具备和所需的条件和经费)
   - [5.1 实验室条件和经费保障](#51-实验室条件和经费保障)
   - [5.2 所需条件和经费](#52-所需条件和经费)
- [6 预计困难及解决方案](#6-预计困难及解决方案)
   - [6.1 预计困难与技术难点](#61-预计困难与技术难点)
   - [6.2 解决方案](#62-解决方案)
- [参考文献](#参考文献)

## 1 课题背景及研究的目的和意义

### 1.1 课题背景

近年来，随着深度学习技术的不断突破，诞生了如 ChatGPT［1］和 BERT［2］等大语言模型（LLM），为生物医学、智能法律、教育辅助等领域带来了革命性的应用潜力。研究表明，LLMs 的性能提升与其本身的参数规模呈现强相关性，例如 GPT-3 使用 1750 亿参数在零样本和少样本任务中表现出超越前代模型的能力［3］，而 GPT-4 通过万亿级参数进一步实现了复杂推理与多模态理解［4］。

在深度学习模型规模呈指数级增长趋势的时代背景下，LLM 的训练过程正面临着严峻的挑战。单机训练这类超大规模模型几乎不可能，因为模型参数和中间激活值的内存需求远远超过了单个 GPU 或 TPU 的内存容量。以 GPT-4 模型为例，其千亿级别的模型参数就需要数百 GB 的存储空间，再加上训练过程中的梯度、优化器状态和中间激活值，总内存需求已经达到 TB 级别。单机显存的不足促使了分布式训练的出现，目前分布式训练已经成为了 LLMs 训练的必要手段［5］［6］，当前主流分布式训练框架如 Megatron-LM［7］、DeepSpeed［8］及 PyTorch-FSDP（Fully Sharded Data Parallelism）［9］已广泛应用于 LLMs 训练。

其中，PyTorch-FSDP 通过将模型参数、梯度和优化器状态分片到多个设备上，显著减少了单个设备的内存需求［9］。然而在分布式训练流程中，节点之间需要频繁同步梯度、参数等数据以保证模型一致性。随着集群规模的扩大和模型参数的增长，通信开销成为制约训练效率的核心瓶颈。在此背景下，研究基于 PyTorch-FSDP 分布式训练的通信数据压缩技术具有重要价值。通过将通信压缩技术与 PyTorch-FSDP 相结合，可以降低通信开销，提高训练效率，并有望降低超大规模模型训练的能耗和碳足迹。

### 1.2 研究的目的和意义

如今，LLM 的模型规模和训练数据正不断扩大，这虽然带来了可观的性能，但也使得通信开销成为新的制约训练效率的核心瓶颈［10］。通信开销的根源在于传统分布式中存在“计算-通信失衡”的问题，且节点间通信还受限于 PCIe 总线带宽、数据传输协议等硬件和软件瓶颈。此外，LLMs 训练中的梯度数据具有高冗余性——梯度向量中约有 80%–90% 的元素绝对值接近零，对模型更新贡献极小，传输完整梯度存在着严重的带宽浪费。

本研究旨在针对 PyTorch-FSDP 分布式训练框架，系统研究通信数据压缩技术的实现方法、优化策略和性能影响，并针对 LLMs 训练中通信开销大、精度与压缩程度平衡难等问题，提出适配 PyTorch-FSDP 分片机制的通信数据压缩方案。

## 2 国内外研究现状及分析

### 2.1 国内外研究现状

国内外学者围绕分布式训练中多机多卡协同以及高效通信已开展了深入探索。

在分布式训练体系架构方面，典型架构包括参数服务器（Parameter Server, PS）［5］、All-Reduce［6］以及去中心化［11］等多种训练体系，以及数据并行［11］、张量并行、流水线并行［12］［13］与 3D 并行［14］等组合策略。而在数据并行的主线下，基于 ZeRO 显存优化技术［15］的 PyTorch-FSDP 作为 PyTorch 生态原生的全分片数据并行技术，在工业界受到了广泛关注。作为一种通用的分布式训练框架，PyTorch-FSDP 通过将模型参数、梯度、优化器状态全部分片到不同的 GPU 节点，解决了传统数据并行（DDP）中单节点内存占用过高的问题。

在高效通信领域，一方面减少通信轮次是降低通信开销的有效方法。实现这一目标的基本思路有两种：一是选择较大的批次大小（batch size）以加快收敛速度；二是执行多轮本地计算后采用周期性通信，即减少各节点之间的同步频率。另一方面，除开减少通信次数之外，还存在一种颇具研究价值的方法，即通信压缩（communication compression）。该方法通过减少每轮迭代中待传输梯度的数据量，或者采用更少比特数对梯度进行编码，从而缩短梯度同步时的通信时间。如何将通信压缩技术合理应用到 PyTorch-FSDP 训练框架下以优化通信效率，是本文的主题。

### 2.2 国内外文献综述及简析

#### 2.2.1 分布式训练框架与并行策略研究现状

近年来，分布式训练与并行化已经成为大模型时代的“基础设施”。

在目前工业界主流训练框架中，Deepak Narayanan 等人提出了综合利用张量并行、流水线并行以及数据并行等多种并行策略组合而成的高效大型 Transformer 语言模型训练框架 Megatron-LM，成功使得大模型的训练过程能够扩展至数千 GPU，并证明了不同的并行策略之间存在非平凡相互作用：策略会影响通信量、内核算效以及流水线气泡等［7］。

Jeff Rasley 等人则利用 ZeRO 内存优化体系提出了 DeepSpeed 深度学习通用分布式训练框架。得益于其关键组件 ZeRO 并行优化器，DeepSpeed 能够显著减少模型并行与数据并行所需要的资源，并大幅提升可训练参数规模。此外 DeepSpeed 还提供了与 PyTorch 兼容的轻量级 API，降低学习成本并提高框架扩展性［8］。

最后，Y. Zhao 等人受 DeepSpeed 的 ZeroRedundancy Optimizer 的启发，通过对模型参数进行全分片的并行化策略实现了一种工业级的大模型训练解决方案，既能达到与 DDP 相当的性能，同时又能支持显著更大的模型规模［9］。

![](assets/wps1-20260111170452-lik1882.jpg)

图 4：PyTorch-FSDP 整体流程图［9］

PyTorch-FSDP 通过切分（shard）稠密参数来扩展到单卡放不下的大模型。更具体地，PyTorch-FSDP 把一个模型实例分解为多个更小的单元并分别处理。在前向与反向计算过程中，PyTorch-FSDP 一次只收集一个单元的完整参数与梯度；其余单元的参数与梯度始终保持为分片状态，且整个训练循环中优化器状态也保持分片。因此，PyTorch-FSDP 的显存需求与“分片后的模型大小 + 最大那个完全收集后的 PyTorch-FSDP 单元的大小”成正比。

如图 1 所示，对于一个 6 层的简单模型，假设 PyTorch-FSDP 将模型分解为三部分：[layer0, layer3]、[layer1, layer2] 与 [layer4, layer5]（在实际应用中，模型的分解方式可以由用户自定义函数来控制）。随后，PyTorch-FSDP 将每一部分都包裹为一个 PyTorch-FSDP 单元并相应地切分参数并存放在分布式节点上。为保证训练过程中模型计算正确，PyTorch-FSDP 需要在对应计算开始前恢复未切分的参数。

以包含 [layer1, layer2] 的 unit1 为例：在前向进入 layer1 之前，PyTorch-FSDP 通过从其他分布式节点上收集分片来得到 layer1 与 layer2 的完整参数，使得 PyTorch-FSDP 能够在本地执行这些层的计算。计算完成之后，PyTorch-FSDP 会立即释放刚收集来的其他节点的分片以降低显存占用。因此，在整个前向过程中，PyTorch-FSDP 任意时刻只需完全实体化一个单元，其他单元都可保持分片。

类似地，在反向计算中，unit1 会在反向即将到达 layer2 之前恢复 layer1 与 layer2 的未切分参数。当自动求导引擎完成这两层的反向计算后，PyTorch-FSDP 立即释放其他节点的分片数据并启动 Reduce-Scatter，对梯度进行规约并重新切分。这样可以保证反向结束后，每个节点仅保留自己那一份参数与梯度的分片。

#### 2.2.2 分布式训练通信优化研究现状

该领域研究主要分为通信次数优化以及通信数据量压缩。

在通信次数优化领域，一个自然的并行化方向是增大 batch size，从而减少模型达到收敛所需要的训练次数，更少的训练轮次意味着训练时各 GPU 节点梯度同步频率降低，从而提高通信效率［16］。但是 N. S. Keskar 和 E. Hoffer 的研究表明，增大 batch size 的训练更容易使模型陷入“尖锐局部极小值”，导致深度神经网络模型的性能难以达到理想水平，此外增大 batch size 也会降低模型的泛化能力［17］。另一种做法是 A. Reisizadeh 等人提出的以周期性聚合与量化为核心并支持部分参与的通信高效联邦学习算法：各节点在与服务器同步前进行若干轮本地更新，再周期性地上传模型，以此显著减少通信轮次与总通信量，但过多的本地更新可能导致模型趋向局部最优［18］。

在通信数据量压缩领域，该方向致力于减少每轮通信中传输的数据量（而非通信次数）。从核心思想出发，通信压缩主要分为两类：稀疏化和量化。

稀疏化的核心思想是仅传输对深度神经网络模型训练收敛具有显著影响的大梯度元素。主流稀疏化方法包括仅发送梯度中最重要的分量（top-k 稀疏化）［19］、通过正负双阈值过滤小梯度的 threshold-v［20］以及对整个梯度张量进行 sketch 映射（Sketched-SGD）［21］或矩阵分解（GradiVeQ）［22］并以此为依据保留重要元素。

![](assets/wps2-20260111170452-9r43qia.png)

图 1：梯度稀疏化示例流程［10］

在量化方面，可以归纳为两大机制：截断表示和码本映射。前者的主要区别在于单个梯度元素所使用的比特数，例如 FP16［23］和 8-bit approximations［24］方法分别使用 16 bit 和 8 bit 表示每个元素，而截断表示的极端情况是 1-bit SGD［25］和 SignSGD［26］，两者均采用单个比特表示每个梯度元素。码本映射方面，主要分为均匀映射与非均匀映射两类：均匀映射的相邻量化等级之间具有相同的区间值，例如 QSGD［27］将原始梯度按量化等级将区间 (0, 1) 等分，再利用随机变量确保量化后梯度的期望与原始期望一致，从而减少对模型收敛的偏差影响；非均匀映射的相邻量化等级区间值则各不相同，注重表示梯度元素的分布情况，例如 NC［28］采用非均匀量化等级，将元素值舍入到与其最接近的两个 2 的整数次幂值之一，并辅以特定概率，使其成为一种无偏的通信量化方法。

![](assets/wps3-20260111170452-s959695.png)

图 2：量化方法一：截断表示［10］

![](assets/wps4-20260111170452-4ierrgc.png)

图 3：量化方法二：码本映射［10］

## 3 主要研究内容及研究方案

### 3.1 研究内容

本研究旨在围绕 PyTorch-FSDP 框架下的通信优化，探索高效的通信数据压缩方法，如梯度量化、稀疏化、低秩近似等策略，减小训练过程中的通信开销，提高 PyTorch-FSDP 分布式训练框架的扩展性和训练效率，并为构建高效、可扩展的大模型训练系统提供理论与实践依据。期望能够在典型大模型的分布式训练中，梯度/参数通信量压缩比达到 2–4 倍，同时在常用数据集上的模型精度下降不超过 1%。

#### 3.1.1 训练过程通信瓶颈分析

构建 PyTorch-FSDP 的通信与计算性能模型，分析 PyTorch-FSDP 训练框架下的通信环节（如 All-Gather、Reduce-Scatter 等），总结 PyTorch-FSDP 训练框架下的通信特征，识别训练过程中的关键瓶颈环节。

#### 3.1.2 压缩算法设计与优化

在现有通信压缩算法基础上（如随机稀疏化、Top-k/Random-k、误差反馈、低比特截断表示等），针对 PyTorch-FSDP 训练框架进行适配，并在真实的大模型训练场景下对比分析不同通信压缩算法的优劣，挑选适配 FSDP 的方案，并针对 PyTorch-FSDP 框架下的通信模式进行改进。

#### 3.1.3 收敛性测试和性能测试

在完成针对 PyTorch-FSDP 框架的通信瓶颈优化之后，需要对优化后训练框架进行多维度测试，如训练过程节点之间通信开销、模型收敛情况等，以确保通信压缩条件下的收敛稳定性以及训练速度提升。

### 3.2 研究方案

针对上述研究内容，制定如下研究流程：

1. 定位 PyTorch-FSDP 中的通信瓶颈：搭建精细化通信监测环境，基于 PyTorch Profiler 与 NVIDIA Nsight Systems 等性能分析工具，对 FSDP 核心通信操作（All-Gather、Reduce-Scatter、All-Reduce）进行全周期数据记录。结合训练全流程构建多维度通信画像：从时间维度分析通信与计算的时序重叠率；从空间维度对比不同模型层通信数据量分布差异；从硬件适配维度记录不同 GPU 集群拓扑（如单机多卡、多机多卡）下通信操作的性能波动。基于画像数据定位核心瓶颈，并分析瓶颈产生的底层原因。
2. 设计并实现适配 FSDP 的压缩算法：复现基础压缩算法并开展基准测试。在 PyTorch-FSDP 框架中集成主流稀疏化与量化算法：稀疏化算法重点实现 Top-k、Random-k 等，并适配 FSDP 分片特性；量化算法则复现如 8-bit（INT8/FP8）、FP16 浮点量化等算法，对训练过程中的梯度/参数传输进行低位宽编码，并在重构后反量化以保证计算精度。复现完成后，以典型模型为测试对象，对比通信量削减比例、训练吞吐量与精度损失等指标，筛选适配 FSDP 的高效压缩方案。
3. 系统性测试：对优化后的训练框架进行多维度、多场景评测，设定分层测试变量与评测指标体系。测试变量涵盖模型规模（小型到大型）、FSDP 分片参数（分片粒度、预取策略等）、压缩算法配置（稀疏率、量化位宽、误差补偿开关等）；评测指标分为三类：通信性能指标（总通信数据量、平均通信延迟、通信耗时占比）、训练效率指标（迭代速度、总训练时长、GPU 内存占用峰值）、模型性能指标（训练收敛曲线、验证集准确率/Perplexity、泛化能力测试等）。以“无压缩 FSDP 训练”为基准组，在相同硬件环境与训练超参下对比不同模型规模的指标差异，形成优化效果报告。

### 3.3 与本仓库实现的对齐（复现说明）

为便于阶段性验证与复现实验，本仓库当前以较小规模模型作为通信压缩对比平台：采用 GPT-2 架构的小模型进行语言建模训练（Wikipedia 文本续写），以降低单次实验成本、缩短迭代周期，并在此基础上逐步验证通信压缩的收益与收敛影响。

当前代码实现中，FSDP 训练入口位于训练脚本中，支持通过 FSDP 通信钩子（comm hook）在 Reduce-Scatter 前后插入梯度量化/反量化逻辑，用于实现 baseline 与压缩方案的对照。评测指标建议至少包含：每步训练耗时（step time）/吞吐（tokens/s）、通信算子耗时占比（profiler 统计）、以及验证集的 loss/perplexity（PPL）。

## 4 进度安排及预期目标

### 4.1 进度安排

本课题研究时间为 2025 年 9 月至 2026 年 4 月（答辩）。结合前期项目经验与学习计划，制定如下进度：

1. 2025 年 9 月至 2025 年 10 月：学习分布式训练基础，掌握常见分布式训练框架；熟悉 PyTorch-FSDP 使用；调研通信压缩相关文献，定位 FSDP 通信瓶颈。
2. 2025 年 10 月至 2026 年 02 月：构建 PyTorch-FSDP 训练过程的通信与计算性能模型，识别训练过程中的关键瓶颈环节。
3. 2026 年 02 月至 2026 年 04 月：在 PyTorch-FSDP 训练框架下设计优化压缩算法，并对优化后训练框架进行性能测试与对比分析。

### 4.2 预期目标

预期目标是研制高效通信的分布式训练框架，本课题预期实现以下目标与成果：

1. 通信压缩率：在典型大模型（如 GPT 类、Transformer 类模型）的分布式训练中，梯度/参数通信量压缩比达到 2–4 倍，在相同带宽条件下显著提升训练吞吐率。
2. 精度保持性：在常用数据集（如 ImageNet、Wikitext、C4）上模型精度下降不超过 1%（Top-1 或 Perplexity 指标）。

## 5 已具备和所需的条件和经费

### 5.1 实验室条件和经费保障

目前分布式训练学习等主要在哈尔滨工业大学（深圳）高性能计算云服务中心上进行，可保证在常见模型和较小数据集上能够正常训练。后续将使用课题组 GPU 资源针对更大模型与更大数据集进行训练与对比实验。

### 5.2 所需条件和经费

后续对优化框架进行系统性测试时，可能需要更多 GPU 资源以支撑超大模型和大规模数据集训练。

## 6 预计困难及解决方案

### 6.1 预计困难与技术难点

课题需要学习的知识较多，涉及 PyTorch 相关 API、NCCL 集群通信、分布式训练基础知识与训练监视工具使用等，各类知识跨度较大，学习难度较高。

### 6.2 解决方案

为提高课题进展速度和效率、保证学习质量，采用“边应用边学习再应用”的方法开展系统学习，并多进行实际操作训练动手能力，也为后期工程化实现打下基础。

## 参考文献

［1］J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre training of deep bidirectional transformers for language understanding,” in the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018.

［2］T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models are few-shot learners,” Advances in Neural Information Processing Systems, vol. 33, pp. 1877–1901, 2020.

［3］Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877–1901.

［4］J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in 2009 IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 2009, pp. 248–255.

［5］M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su, “Scaling distributed machine learning with the parameter server,” in 11th USENIX Symposium on Operating Systems Design and Implementation, 2014, pp. 583–598.

［6］A. Gibiansky, “Bringing HPC techniques to deep learning,” Baidu Research, Tech. Rep., 2017.

［7］D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro et al., “Efficient large-scale language model training on GPU clusters using megatron-lm,” in International Conference for High Performance Computing, Networking, Storage and Analysis, 2021, pp. 1–15.

［8］J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, “DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters,” in the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 3505–3506.

［9］Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer et al., “Pytorch FSDP: experiences on scaling fully sharded data parallel,” arXiv preprint arXiv:2304.11277, 2023.

［10］Wang Z, Wen M, Xu Y, Zhou Y, Wang JH, Zhang L. “Communication compression techniques in distributed deep learning: A survey.” Journal of Systems Architecture, 2023 Sep 1;142:102927.

［11］Q. Luo, J. He, Y. Zhuo, and X. Qian, “Prague: High-performance heterogeneity-aware asynchronous decentralized training,” in Twenty Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, 2020, pp. 401–416.

［12］Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu et al., “GPipe: Efficient training of giant neural networks using pipeline parallelism,” Advances in Neural Information Processing Systems, vol. 32, 2019.

［13］A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger, and P. Gibbons, “PipeDream: Fast and efficient pipeline parallel DNN training,” arXiv preprint arXiv:1806.03377, 2018.

［14］Z. Lai, S. Li, X. Tang, K. Ge, W. Liu, Y. Duan, L. Qiao, and D. Li, “Merak: An efficient distributed DNN training framework with automated 3D parallelism for giant foundation models,” IEEE Transactions on Parallel and Distributed Systems, vol. 34, no. 5, pp. 1466–1478, 2023.

［15］S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “ZeRO: Memory optimizations toward training trillion parameter models,” in the International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, 2020, pp. 1–16.

［16］Rui Liu and Barzan Mozafari. 2022. “Communication-efficient distributed learning for large batch optimization.” In International Conference on Machine Learning. PMLR, 13925–13946.

［17］N. S. Keskar, J. Nocedal, P. T. P. Tang, D. Mudigere, M. Smelyanskiy, “On large batch training for deep learning: Generalization gap and sharp minima,” in 5th International Conference on Learning Representations, ICLR 2017, 2017.

［18］A. Reisizadeh, A. Mokhtari, H. Hassani, A. Jadbabaie, R. Pedarsani, “Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization,” in International Conference on Artificial Intelligence and Statistics, PMLR, 2020, pp. 2021–2031.

［19］S. U. Stich, J.-B. Cordonnier, M. Jaggi, “Sparsified SGD with memory,” Advances in Neural Information Processing Systems 31 (2018).

［20］A. Dutta, E. H. Bergou, A. M. Abdelmoniem, C.-Y. Ho, A. N. Sahu, M. Canini, P. Kalnis, “On the discrepancy between the theoretical analysis and practical implementations of compressed communication for distributed deep learning,” in Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, No. 04, 2020, pp. 3817–3824.

［21］N. Ivkin, D. Rothchild, E. Ullah, I. Stoica, R. Arora, et al., “Communication efficient distributed SGD with sketching,” Advances in Neural Information Processing Systems 32 (2019).

［22］M. Yu, Z. Lin, K. Narra, S. Li, Y. Li, N. S. Kim, A. Schwing, M. Annavaram, S. Avestimehr, “Gradiveq: Vector quantization for bandwidth-efficient gradient aggregation in distributed CNN training,” Advances in Neural Information Processing Systems 31 (2018).

［23］P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., “Mixed precision training,” arXiv preprint arXiv:1710.03740, 2017.

［24］T. Dettmers, “8-bit approximations for parallelism in deep learning,” arXiv preprint arXiv:1511.04561, 2015.

［25］F. Seide, H. Fu, J. Droppo, G. Li, D. Yu, “1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs,” in Fifteenth Annual Conference of the International Speech Communication Association, 2014.

［26］J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, A. Anandkumar, “signSGD: Compressed optimisation for non-convex problems,” in International Conference on Machine Learning, PMLR, 2018, pp. 560–569.

［27］D. Alistarh, D. Grubic, J. Li, R. Tomioka, M. Vojnovic, “QSGD: Communication efficient SGD via gradient quantization and encoding,” Advances in Neural Information Processing Systems 30 (2017).

［28］S. Horvóth, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, P. Richtárik, “Natural compression for distributed deep learning,” in Mathematical and Scientific Machine Learning, PMLR, 2022, pp. 129–141.
