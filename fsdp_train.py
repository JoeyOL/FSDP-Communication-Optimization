import os
import torch
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import StateDictType
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
from torch.utils.data import DataLoader, DistributedSampler
from transformers import (
    get_linear_schedule_with_warmup,
    DataCollatorForLanguageModeling
)
from pathlib import Path
import argparse
import functools
import torch.distributed as dist
import random
import numpy as np
from transformers import (
    DataCollatorForLanguageModeling
)
from transformers.models.gpt2.modeling_gpt2 import GPT2Block
from create_model import load_model, load_tokenizer
from data_base import WikipediaDataset
from train_func import train_epoch_with_monitoring
from logger import logger
from comm_compress import list_methods, make_comm_hook
from torch.distributed.checkpoint.state_dict import (
    StateDictOptions,
    get_state_dict,
)


def set_seed(seed: int) -> None:
    """å°½é‡ä¿è¯å¯å¤ç°ï¼ˆæ³¨æ„ï¼šå¤š GPU/FSDP ä»å¯èƒ½å­˜åœ¨éç¡®å®šæ€§ç®—å­ï¼‰ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        rank = 0
        world_size = 1
        local_rank = 0
    
    # æ€»æ˜¯åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼Œå³ä½¿æ˜¯å•GPUä¹Ÿéœ€è¦ï¼ˆFSDPè¦æ±‚ï¼‰
    if world_size > 1:
        dist.init_process_group(backend='nccl')
        torch.cuda.set_device(local_rank)
    else:
        # å•GPUç¯å¢ƒä¸‹ä¹Ÿéœ€è¦åˆå§‹åŒ–è¿›ç¨‹ç»„
        if torch.cuda.is_available():
            torch.cuda.set_device(local_rank)
        # ä½¿ç”¨å•æœºæ¨¡å¼åˆå§‹åŒ–è¿›ç¨‹ç»„
        port = 12356
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = str(port)
        dist.init_process_group(backend='gloo', 
                              rank=0, world_size=1)
    
    return rank, world_size, local_rank

def main():
    parser = argparse.ArgumentParser(description='LLaMA-7B FSDP è®­ç»ƒ')
    parser.add_argument('--model_path', type=str, default='/root/llama-7b', help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--data_path', type=str, default='/root/llama-7b/datasets/wikipedia_en_10mb.json', help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='/root/llama-7b/fsdp_output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=2, help='æ‰¹é‡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-6, help='å­¦ä¹ ç‡')
    parser.add_argument('--num_epochs', type=int, default=3, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--max_length', type=int, default=512, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--warmup_steps', type=int, default=100, help='é¢„çƒ­æ­¥æ•°')
    parser.add_argument('--save_steps', type=int, default=500, help='ä¿å­˜é—´éš”')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    parser.add_argument('--log_interval', type=int, default=10, help='æ—¥å¿—è¾“å‡ºé—´éš”')
    parser.add_argument('--eval_steps', type=int, default=None, help='è¯„ä¼°é—´éš”æ­¥æ•°')
    parser.add_argument('--dataloader_num_workers', type=int, default=2, help='æ•°æ®åŠ è½½å™¨workeræ•°é‡')
    parser.add_argument('--run_name', type=str, default='llama7b-fsdp-wiki', help='è¿è¡Œåç§°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--dataset_shard_size', type=int, default=2000, help='é¢„åˆ†è¯ç¼“å­˜åˆ†ç‰‡å¤§å°ï¼ˆæ¡æ•°ï¼‰ï¼Œç”¨äºå¤§ JSON æ–‡ä»¶')
    parser.add_argument('--dataset_max_samples', type=int, default=0, help='æœ€å¤šåŠ è½½/é¢„åˆ†è¯å¤šå°‘æ¡æ ·æœ¬ï¼ˆ0è¡¨ç¤ºå…¨é‡ï¼‰ï¼Œç”¨äºå¿«é€Ÿè‡ªæ£€')

    # --- Step2ï¼šé€šä¿¡å‹ç¼©æ¨¡å—åŒ–ï¼ˆFSDP comm hookï¼‰ ---
    parser.add_argument(
        '--comm_compress',
        type=str,
        default='none',
        help='é€šä¿¡å‹ç¼©æ–¹æ³•ï¼šnone/int8/...ï¼ˆå¯ç”¨æ–¹æ³•è§ comm_compress.list_methods()ï¼‰',
    )
    parser.add_argument(
        '--comm_config_json',
        type=str,
        default='',
        help='å‹ç¼©æ–¹æ³•é…ç½®ï¼ˆJSON å¯¹è±¡å­—ç¬¦ä¸²ï¼‰ï¼Œä¾‹å¦‚ï¼š{"num_bits":8}',
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
    rank, world_size, local_rank = setup_distributed()

    # å¯å¤ç°æ€§ï¼ˆåœ¨åˆå§‹åŒ–è¿›ç¨‹ç»„åè°ƒç”¨ï¼Œä¿è¯å„ rank éƒ½è®¾ç½®ï¼‰
    set_seed(args.seed + rank)
    
    logger.info(f"ğŸ¯ Rank {rank} å¼€å§‹åŠ è½½æ¨¡å‹...")
    logger.info(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    logger.info(f"å½“å‰è®¾å¤‡: cuda:{local_rank}")
    
    tokenizer = load_tokenizer()
    
    model = load_model(tokenizer)
    model = model.to(f'cuda:{local_rank}')
    
    # ä¼˜åŒ–çš„ FSDP é…ç½® - æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–
    logger.info("åˆ›å»ºFSDPåŒ…è£…...")
    # ä¼˜åŒ–çš„ FSDP é…ç½®
    model = FSDP(model,
        device_id=local_rank,
        auto_wrap_policy = functools.partial(
            transformer_auto_wrap_policy,
            transformer_layer_cls={
                GPT2Block,
            }
        )
    )

    # --- Step2ï¼šæŒ‰å‚æ•°æ³¨å†Œé€šä¿¡å‹ç¼© hook ---
    method = (args.comm_compress or 'none').strip().lower()
    if method not in ('none', 'off', 'disable', 'disabled'):
        if world_size <= 1:
            if rank == 0:
                logger.warning(f"å·²æŒ‡å®š --comm_compress={method}ï¼Œä½†å½“å‰ world_size=1ï¼Œä¸ä¼šæ³¨å†Œ comm hookã€‚")
        else:
            if rank == 0:
                logger.info(f"ğŸ”§ æ³¨å†Œé€šä¿¡å‹ç¼© hook: {method}")
                logger.info(f"å¯ç”¨æ–¹æ³•: {list(list_methods().keys())}")
            state, hook = make_comm_hook(method, args.comm_config_json or None)
            if hook is not None:
                model.register_comm_hook(state, hook)
                if rank == 0:
                    logger.info("âœ… é€šä¿¡å‹ç¼© hook æ³¨å†ŒæˆåŠŸ")
    
    logger.info(f"âœ… Rank {rank} æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ•°æ®é›†
    logger.info("åŠ è½½æ•°æ®é›†...")
    dataset = WikipediaDataset(
        args.data_path,
        tokenizer,
        args.max_length,
        shard_size=args.dataset_shard_size,
        max_samples=args.dataset_max_samples,
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å‡å°‘å†…å­˜ä½¿ç”¨
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) if world_size > 1 else None
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False
    )
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        sampler=sampler,
        collate_fn=data_collator,
        num_workers=args.dataloader_num_workers,
        pin_memory=True
    )


    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
    
    # åˆ›å»ºå¸¦é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ (å…³é”®ï¼)
    total_steps = (len(dataloader) // args.gradient_accumulation_steps) * args.num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=total_steps
    )
    logger.info(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}, é¢„çƒ­æ­¥æ•°: {args.warmup_steps}")
    logger.info(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    logger.info(f"æ¯ä¸ªepochæ­¥æ•°: {len(dataloader)}")
        
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.num_epochs):
        if sampler is not None:
            sampler.set_epoch(epoch)
        
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss = train_epoch_with_monitoring(
            model, dataloader, optimizer, scheduler, epoch + 1, rank, world_size, args, 
        )
        
        if rank == 0:
            logger.info(f"Epoch {epoch + 1}/{args.num_epochs}, å¹³å‡æŸå¤±: {avg_loss:.4f}")
    
    logger.info(f"Rank {rank} æ­£åœ¨å‚ä¸æ”¶é›†çŠ¶æ€å­—å…¸...")
    # torch==2.5.1 çš„ StateDictOptions ä¸æ”¯æŒ rank0_onlyã€‚
    # ç”¨ broadcast_from_rank0ï¼šå…ˆç”± rank0 æ”¶é›†å®Œæ•´ state_dictï¼Œå†å¹¿æ’­åˆ°å…¶ä»– rankï¼Œ
    # åŒæ—¶å¯ç”¨ cpu_offload å°† state_dict æ”¾åˆ° CPUï¼Œé™ä½ GPU å³°å€¼æ˜¾å­˜ã€‚
    # æ³¨æ„ï¼šget_state_dict å†…éƒ¨åŒ…å«é›†ä½“é€šä¿¡ï¼Œå¿…é¡»æ‰€æœ‰ rank éƒ½æ‰§è¡Œåˆ°è¿™é‡Œã€‚
    options = StateDictOptions(
        full_state_dict=True,
        cpu_offload=True,
        broadcast_from_rank0=True,
    )
    full_state_dict = get_state_dict(model, optimizer, options=options)
    
    if rank == 0:
        logger.info("è®­ç»ƒå®Œæˆ! Rank 0 å¼€å§‹ä¿å­˜æ¨¡å‹...")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_dir = Path(args.output_dir) / "final_model"
        final_dir.mkdir(parents=True, exist_ok=True)
        
        # ä»è¿”å›å€¼ä¸­æå–æ¨¡å‹çŠ¶æ€ï¼ˆä¸åŒ torch ç‰ˆæœ¬è¿”å›ç»“æ„å¯èƒ½ä¸åŒï¼‰
        # - å¯èƒ½æ˜¯ dict: {"model": ..., "optimizer": ...}
        # - ä¹Ÿå¯èƒ½æ˜¯ tuple: (model_state_dict, optim_state_dict)
        if isinstance(full_state_dict, dict):
            model_state_dict = full_state_dict["model"]
        elif isinstance(full_state_dict, tuple) and len(full_state_dict) >= 1:
            model_state_dict = full_state_dict[0]
        else:
            raise TypeError(
                f"get_state_dict è¿”å›äº†ä¸æ”¯æŒçš„ç±»å‹: {type(full_state_dict)}"
            )
        logger.info("çŠ¶æ€å­—å…¸åœ¨ Rank 0 ä¸Šæ”¶é›†å®Œæˆã€‚")
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        torch.save(model_state_dict, final_dir / "pytorch_model.bin")
        tokenizer.save_pretrained(final_dir)

    # é˜²æ­¢ rank0 ä¿å­˜æ—¶é—´è¾ƒé•¿å¯¼è‡´å…¶ä»– rank æå‰é€€å‡ºï¼Œå¼•å‘åç»­é€šä¿¡/é”€æ¯é˜¶æ®µå¼‚å¸¸
    if dist.is_initialized():
        dist.barrier()
    
    dist.barrier()
    
    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    if dist.is_initialized():
        dist.destroy_process_group()

if __name__ == "__main__":
    main()
