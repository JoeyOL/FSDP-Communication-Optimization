import os
import torch
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import StateDictType
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
from torch.utils.data import DataLoader, DistributedSampler
from transformers import (
    get_linear_schedule_with_warmup,
    DataCollatorForLanguageModeling
)
from pathlib import Path
import argparse
import functools
import torch.distributed as dist
import random
import numpy as np
from transformers import (
    DataCollatorForLanguageModeling
)
from transformers.models.gpt2.modeling_gpt2 import GPT2Block
from create_model import load_model, load_tokenizer
from data_base import WikipediaDataset
from train_func import train_epoch_with_monitoring
from logger import logger
from torch.distributed.checkpoint.state_dict import (
    StateDictOptions,
    get_state_dict,
)


def set_seed(seed: int) -> None:
    """å°½é‡ä¿è¯å¯å¤ç°ï¼ˆæ³¨æ„ï¼šå¤š GPU/FSDP ä»å¯èƒ½å­˜åœ¨éç¡®å®šæ€§ç®—å­ï¼‰ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


class GradQuantState:
    def __init__(self, num_bits: int = 8):
        self.num_bits = num_bits

def fsdp_quantized_comm_hook(
    state: GradQuantState,
    full_flat_grad: torch.Tensor,
    shard_out: torch.Tensor,
) -> None:
    """
    FSDP é€šä¿¡é’©å­ï¼Œæ ¹æ®å®˜æ–¹æ–‡æ¡£ä¿®æ­£ã€‚
    åœ¨ reduce-scatter å‰è¿›è¡Œ int8 å¯¹ç§°é‡åŒ–ï¼Œé€šä¿¡ååé‡åŒ–ï¼Œå¹¶å°†ç»“æœå†™å…¥ shard_outã€‚
    æ­¤å‡½æ•°ä¸è¿”å›å€¼ã€‚

    Args:
        state (GradQuantState): åŒ…å«é‡åŒ–ä½æ•°çš„è‡ªå®šä¹‰çŠ¶æ€å¯¹è±¡ã€‚
        full_flat_grad (torch.Tensor): FSDP ä¼ å…¥çš„å®Œæ•´ã€æ‰å¹³åŒ–çš„æ¢¯åº¦ã€‚
        shard_out (torch.Tensor): ä¸€ä¸ªé¢„å…ˆåˆ†é…å¥½çš„ç¼“å†²åŒºï¼Œç”¨äºå­˜å‚¨æ­¤ rank çš„æ¢¯åº¦åˆ†ç‰‡ç»“æœã€‚
    """
    assert isinstance(state, GradQuantState)
    pg = dist.group.WORLD  # ä½¿ç”¨é»˜è®¤çš„å…¨å±€è¿›ç¨‹ç»„
    world_size = dist.get_world_size(pg)

    # å¦‚æœåªæœ‰ä¸€ä¸ª GPUï¼Œåˆ™æ— éœ€é€šä¿¡ï¼Œç›´æ¥å¤åˆ¶æ¢¯åº¦åˆ†ç‰‡
    if world_size == 1:
        shard_out.copy_(full_flat_grad)
        return

    # å±•å¹³æ¢¯åº¦ (è™½ç„¶å·²æ˜¯æ‰å¹³çš„ï¼Œä½†ç¡®ä¿ view æ­£ç¡®)
    g = full_flat_grad.contiguous().view(-1)
    numel = g.numel()
    assert numel % world_size == 0, f"æ‰å¹³æ¢¯åº¦å¤§å° {numel} å¿…é¡»èƒ½è¢« world_size {world_size} æ•´é™¤"

    # 1) å…¨å±€ max_abs åŒæ­¥ï¼Œä»¥ç¡®å®šç»Ÿä¸€çš„é‡åŒ–å°ºåº¦
    local_max = g.abs().max().to(torch.float32)
    global_max = local_max.clone()
    dist.all_reduce(global_max, op=dist.ReduceOp.MAX, group=pg)

    # 2) å¯¹ç§°é‡åŒ–åˆ° int8 (å¸¦æœ‰ world_size å®‰å…¨ä¸Šé™ï¼Œä¿è¯ int8 è§„çº¦ä¸æº¢å‡º)
    Q = 127
    # è®¡ç®—æ¯ä¸ª rank çš„é‡åŒ–èŒƒå›´ï¼Œç¡®ä¿æ‰€æœ‰ rank çš„é‡åŒ–å€¼ç›¸åŠ åä¸ä¼šè¶…è¿‡ int8 çš„èŒƒå›´
    Qr = max(1, Q // world_size)
    scale = Qr / torch.clamp(global_max, min=1e-8)   # x * scale çš„èŒƒå›´åœ¨ [-Qr, Qr]
    q = torch.clamp((g * scale).round(), -Qr, Qr).to(torch.int8)
    
    temp_shard_out = torch.empty_like(shard_out, dtype=torch.int8)

    # 3) ç›´æ¥ä½¿ç”¨ int8 ç±»å‹è¿›è¡Œ reduce-scatter(sum) é€šä¿¡
    if hasattr(dist, "reduce_scatter_tensor"):
        # PyTorch è¾ƒæ–°ç‰ˆæœ¬
        dist.reduce_scatter_tensor(temp_shard_out, q, op=dist.ReduceOp.SUM, group=pg)
    else:
        # å…¼å®¹è¾ƒè€ç‰ˆæœ¬
        chunks = list(q.chunk(world_size, dim=0))
        dist.reduce_scatter(temp_shard_out, chunks, op=dist.ReduceOp.SUM, group=pg)

    # 4) åé‡åŒ–å¹¶æ±‚å¹³å‡ï¼Œç„¶åå°†æœ€ç»ˆç»“æœå†™å…¥ shard_out
    deq_sum = temp_shard_out.float() / scale  # æ¢å¤åˆ° float ç±»å‹ï¼Œè¿‘ä¼¼äºåŸå§‹æ¢¯åº¦çš„å’Œ
    deq_avg = (deq_sum / float(world_size)).to(full_flat_grad.dtype) # æ±‚å¹³å‡å¹¶è½¬å›åŸå§‹ç²¾åº¦
    shard_out.copy_(deq_avg)
    return




def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        rank = 0
        world_size = 1
        local_rank = 0
    
    # æ€»æ˜¯åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼Œå³ä½¿æ˜¯å•GPUä¹Ÿéœ€è¦ï¼ˆFSDPè¦æ±‚ï¼‰
    if world_size > 1:
        dist.init_process_group(backend='nccl')
        torch.cuda.set_device(local_rank)
    else:
        # å•GPUç¯å¢ƒä¸‹ä¹Ÿéœ€è¦åˆå§‹åŒ–è¿›ç¨‹ç»„
        if torch.cuda.is_available():
            torch.cuda.set_device(local_rank)
        # ä½¿ç”¨å•æœºæ¨¡å¼åˆå§‹åŒ–è¿›ç¨‹ç»„
        port = 12356
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = str(port)
        dist.init_process_group(backend='gloo', 
                              rank=0, world_size=1)
    
    return rank, world_size, local_rank

def main():
    parser = argparse.ArgumentParser(description='LLaMA-7B FSDP è®­ç»ƒ')
    parser.add_argument('--model_path', type=str, default='/root/llama-7b', help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--data_path', type=str, default='/root/llama-7b/datasets/wikipedia_en_10mb.json', help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='/root/llama-7b/fsdp_output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=2, help='æ‰¹é‡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-6, help='å­¦ä¹ ç‡')
    parser.add_argument('--num_epochs', type=int, default=3, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--max_length', type=int, default=512, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--warmup_steps', type=int, default=100, help='é¢„çƒ­æ­¥æ•°')
    parser.add_argument('--save_steps', type=int, default=500, help='ä¿å­˜é—´éš”')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    parser.add_argument('--log_interval', type=int, default=10, help='æ—¥å¿—è¾“å‡ºé—´éš”')
    parser.add_argument('--eval_steps', type=int, default=None, help='è¯„ä¼°é—´éš”æ­¥æ•°')
    parser.add_argument('--dataloader_num_workers', type=int, default=2, help='æ•°æ®åŠ è½½å™¨workeræ•°é‡')
    parser.add_argument('--run_name', type=str, default='llama7b-fsdp-wiki', help='è¿è¡Œåç§°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--dataset_shard_size', type=int, default=2000, help='é¢„åˆ†è¯ç¼“å­˜åˆ†ç‰‡å¤§å°ï¼ˆæ¡æ•°ï¼‰ï¼Œç”¨äºå¤§ JSON æ–‡ä»¶')
    parser.add_argument('--dataset_max_samples', type=int, default=0, help='æœ€å¤šåŠ è½½/é¢„åˆ†è¯å¤šå°‘æ¡æ ·æœ¬ï¼ˆ0è¡¨ç¤ºå…¨é‡ï¼‰ï¼Œç”¨äºå¿«é€Ÿè‡ªæ£€')
    
    args = parser.parse_args()
    
    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
    rank, world_size, local_rank = setup_distributed()

    # å¯å¤ç°æ€§ï¼ˆåœ¨åˆå§‹åŒ–è¿›ç¨‹ç»„åè°ƒç”¨ï¼Œä¿è¯å„ rank éƒ½è®¾ç½®ï¼‰
    set_seed(args.seed + rank)
    
    logger.info(f"ğŸ¯ Rank {rank} å¼€å§‹åŠ è½½æ¨¡å‹...")
    logger.info(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    logger.info(f"å½“å‰è®¾å¤‡: cuda:{local_rank}")
    
    tokenizer = load_tokenizer()
    
    model = load_model(tokenizer)
    model = model.to(f'cuda:{local_rank}')
    
    # ä¼˜åŒ–çš„ FSDP é…ç½® - æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–
    logger.info("åˆ›å»ºFSDPåŒ…è£…...")
    # ä¼˜åŒ–çš„ FSDP é…ç½®
    model = FSDP(model,
        device_id=local_rank,
        auto_wrap_policy = functools.partial(
            transformer_auto_wrap_policy,
            transformer_layer_cls={
                GPT2Block,
            }
        )
    )
    #  # --- æ–°å¢ï¼šæ³¨å†Œæ¢¯åº¦é‡åŒ–é€šä¿¡é’©å­ ---
    # if world_size > 1:  # åªåœ¨å¤šGPUæ—¶æ³¨å†Œ
    #     logger.info("ğŸ”§ æ³¨å†Œæ¢¯åº¦é‡åŒ–é€šä¿¡é’©å­...")
    #     model.register_comm_hook(GradQuantState(num_bits=8),
    #                              fsdp_quantized_comm_hook)
    #     logger.info("âœ… æ¢¯åº¦é‡åŒ–é’©å­æ³¨å†ŒæˆåŠŸ - æ¢¯åº¦å°†åœ¨é€šä¿¡æ—¶è‡ªåŠ¨é‡åŒ–ä¸º8ä½")
    
    logger.info(f"âœ… Rank {rank} æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ•°æ®é›†
    logger.info("åŠ è½½æ•°æ®é›†...")
    dataset = WikipediaDataset(
        args.data_path,
        tokenizer,
        args.max_length,
        shard_size=args.dataset_shard_size,
        max_samples=args.dataset_max_samples,
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å‡å°‘å†…å­˜ä½¿ç”¨
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) if world_size > 1 else None
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False
    )
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        sampler=sampler,
        collate_fn=data_collator,
        num_workers=args.dataloader_num_workers,
        pin_memory=True
    )


    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
    
    # åˆ›å»ºå¸¦é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ (å…³é”®ï¼)
    total_steps = (len(dataloader) // args.gradient_accumulation_steps) * args.num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=total_steps
    )
    logger.info(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}, é¢„çƒ­æ­¥æ•°: {args.warmup_steps}")
    logger.info(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    logger.info(f"æ¯ä¸ªepochæ­¥æ•°: {len(dataloader)}")
        
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.num_epochs):
        if sampler is not None:
            sampler.set_epoch(epoch)
        
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss = train_epoch_with_monitoring(
            model, dataloader, optimizer, scheduler, epoch + 1, rank, world_size, args, 
        )
        
        if rank == 0:
            logger.info(f"Epoch {epoch + 1}/{args.num_epochs}, å¹³å‡æŸå¤±: {avg_loss:.4f}")
    
    logger.info(f"Rank {rank} æ­£åœ¨å‚ä¸æ”¶é›†çŠ¶æ€å­—å…¸...")
    # torch==2.5.1 çš„ StateDictOptions ä¸æ”¯æŒ rank0_onlyã€‚
    # ç”¨ broadcast_from_rank0ï¼šå…ˆç”± rank0 æ”¶é›†å®Œæ•´ state_dictï¼Œå†å¹¿æ’­åˆ°å…¶ä»– rankï¼Œ
    # åŒæ—¶å¯ç”¨ cpu_offload å°† state_dict æ”¾åˆ° CPUï¼Œé™ä½ GPU å³°å€¼æ˜¾å­˜ã€‚
    # æ³¨æ„ï¼šget_state_dict å†…éƒ¨åŒ…å«é›†ä½“é€šä¿¡ï¼Œå¿…é¡»æ‰€æœ‰ rank éƒ½æ‰§è¡Œåˆ°è¿™é‡Œã€‚
    options = StateDictOptions(
        full_state_dict=True,
        cpu_offload=True,
        broadcast_from_rank0=True,
    )
    full_state_dict = get_state_dict(model, optimizer, options=options)
    
    if rank == 0:
        logger.info("è®­ç»ƒå®Œæˆ! Rank 0 å¼€å§‹ä¿å­˜æ¨¡å‹...")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_dir = Path(args.output_dir) / "final_model"
        final_dir.mkdir(parents=True, exist_ok=True)
        
        # ä»è¿”å›å€¼ä¸­æå–æ¨¡å‹çŠ¶æ€ï¼ˆä¸åŒ torch ç‰ˆæœ¬è¿”å›ç»“æ„å¯èƒ½ä¸åŒï¼‰
        # - å¯èƒ½æ˜¯ dict: {"model": ..., "optimizer": ...}
        # - ä¹Ÿå¯èƒ½æ˜¯ tuple: (model_state_dict, optim_state_dict)
        if isinstance(full_state_dict, dict):
            model_state_dict = full_state_dict["model"]
        elif isinstance(full_state_dict, tuple) and len(full_state_dict) >= 1:
            model_state_dict = full_state_dict[0]
        else:
            raise TypeError(
                f"get_state_dict è¿”å›äº†ä¸æ”¯æŒçš„ç±»å‹: {type(full_state_dict)}"
            )
        logger.info("çŠ¶æ€å­—å…¸åœ¨ Rank 0 ä¸Šæ”¶é›†å®Œæˆã€‚")
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        torch.save(model_state_dict, final_dir / "pytorch_model.bin")
        tokenizer.save_pretrained(final_dir)

    # é˜²æ­¢ rank0 ä¿å­˜æ—¶é—´è¾ƒé•¿å¯¼è‡´å…¶ä»– rank æå‰é€€å‡ºï¼Œå¼•å‘åç»­é€šä¿¡/é”€æ¯é˜¶æ®µå¼‚å¸¸
    if dist.is_initialized():
        dist.barrier()
    
    dist.barrier()
    
    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    if dist.is_initialized():
        dist.destroy_process_group()

if __name__ == "__main__":
    main()
