# Source: Communication compression techniques in distributed deep learning_ A survey.pdf
# Pages: 26



--- Page 1 ---
Journal of Systems Architecture 142 (2023) 102927
Available online 22 June 2023
1383-7621/Â© 2023 Elsevier B.V. All rights reserved.
Contents lists available at ScienceDirect
Journal of Systems Architecture
journal homepage: www.elsevier.com/locate/sysarc
Communication compression techniques in distributed deep learning: A
surveyâœ©
Zeqin Wanga, Ming Wena, Yuedong Xua,âˆ—, Yipeng Zhoub, Jessie Hui Wangc, Liang Zhangd
a School of Information Science and Technology, Fudan University, Shanghai, 200433, China
b FSE, School of Computing, Macquarie University, Macquarie Park, NSW 2113, Australia
c Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, 100084, China
d Huawei Technologies, Nanjing Research Center, Nanjing, 210096, China
A R T I C L E I N F O
Keywords:
Distributed deep learning
Communication compression
Sparsification
Quantization
A B S T R A C T
Nowadays, the training data and neural network models are getting increasingly large. The training time of
deep learning will become unbearably long on a single machine. To reduce the computation and storage
burdens, distributed deep learning has been put forward to collaboratively train a large neural network model
with multiple computing nodes in parallel. The unbalanced development of computation and communication
capabilities has led to training time being dominated by communication time, making the communication
overhead a major challenge toward efficient distributed deep learning. Communication compression is an
effective method to alleviate communication overhead, and it has evolved from simple random sparsification
or quantization to versatile strategies or data structures. In this survey, existing communication compression
techniques are reviewed and classified to provide a birdâ€™s eye view. The main properties of each class of
compression methods are analyzed, and their applications or theoretical convergence are described if necessary.
This survey is potentially helpful for researchers and engineers to understand the up-to-date achievements on
the communication compression techniques that accelerate the training of large deep learning models.
1. Introduction
Deep learning is an important branch of machine learning that uses
multiple neural processing layers with complex structure or consisting
of multiple nonlinear transformations to extract high-level features of
data [1]. With the rapid development of big data and computing power,
deep learning has witnessed a tremendous growth of applications in
image processing [ 2], natural language processing [ 3], speech recog-
nition [ 4], data mining [ 5] etc., significantly outperforming domain
specific algorithms and classical machine learning methods. A recent
surge of interest is employing deep learning to address science problems
in chemistry, biology, physics and mathematics [ 6].
Todayâ€™s deep learning models are usually gigantic in terms of their
large number of neural network layers and huge amount of model
parameters, combined with the huge volume of training data. For
instance, BERT-base and BERT-large models contain more than 110
million and 340 million parameters respectively [ 7], and GPT-4 even
owns 175 billion parameters. The ImageNet dataset contains 14 million
âœ© This work was supported in part by the Natural Science Foundation of China under Grant Grant 62072117, the Shanghai Natural Science Foundation under
the Grant 22ZR1407000 and Huawei collaborative project on stochastic network modeling.
âˆ— Corresponding author.
E-mail addresses: zeqinwang21@m.fudan.edu.cn (Z. Wang), mwen19@fudan.edu.cn (M. Wen), ydxu@fudan.edu.cn (Y. Xu), yipeng.zhou@mq.edu.au
(Y. Zhou), jessiewang@tsinghua.edu.cn (J.H. Wang), zhangliang1@huawei.com (L. Zhang).
images for image recognition [ 8] and WMT-2014 contains 36 million
bilingual pairs for natural language processing [9]. A single machine is
nearly impossible to load all the training data or the model to its local
memory. The computing power of a machine also hampers the speed of
training large deep learning models. With stochastic gradient descent
(SGD) method as the training algorithm, computing the gradients by
traversing all the data samples in one epoch is time-consuming. In
addition, the training data are dispersed in multiple sites that do not
allow the exchange or aggregation of raw data due to privacy concerns.
Therefore, an inevitable trend is to perform learning calculations in par-
allel on distributed computing clusters for large deep neural networks
(DNNs).
Distributed DNN training splits the centralized computation into
multiple parallel computations. Data parallelism is the most common
distributed mode. Each machine has access to a specific partition of
the data set. A typical distributed training architecture is shown in
Fig. 1 that consists of a central server (namely parameter server) and
https://doi.org/10.1016/j.sysarc.2023.102927
Received 4 March 2023; Received in revised form 18 May 2023; Accepted 17 June 2023


--- Page 2 ---
Journal of Systems Architecture 142 (2023) 102927
2
Z. Wang et al.
Fig. 1. Distributed deep learning.
multiple local workers. Each worker computes the local gradient based
on its data samples and pushes it to the server for aggregation. After the
central server generates a global model from received local gradients,
all the workers pull this new model and start the next training round.
This procedure repeats many rounds until the DNN model converges.
The distributed training relieves the burden of centralized computation,
while eliciting a huge communication overhead between the parameter
server and the local workers. When the DNN model is large and the
bandwidth is limited, the communication time is comparable to the
computation time, and even overwhelms the latter, thus undermining
the benefit of distributed learning.
Communication reduction is one of the primary challenges in im-
proving the efficiency of distributed deep learning. Usually, two direct
approaches are adopted. First, each worker chooses a batch of data
samples at an iteration in the SGD based training. The large batch
size significantly reduces the number of iterations needed to reach the
certain objective, thus diminishing the communication load. Second,
each worker periodically pushes its gradient to the parameter server
after performing multiple local iterations instead of every iteration.
Despite their advantages in communication reduction, a large batch
size usually causes the DNN training to converge to an unsatisfactory
local minimum, and a long update interval might deviate the local
models from the global model that slows down the convergence rate.
An intriguing property of distributed DNN training is that it is tolerable
to the imperfection of received local models. On one hand, most of
the gradients in a local model are extremely small, and their influence
on the convergence rate is very likely marginal. On the other hand,
if the fewer number of bits is used to represent each parameter, the
performance of the DNN model and the convergence rate measured
in the training rounds might not be affected under certain situations.
These observations give rise to a cluster of communication reduction
techniques, namely, â€˜â€˜communication compressionâ€™â€™. Based on its core
idea, communication compression techniques are roughly categorized
into two groups, the sparsification and the quantization of gradients to
be transferred.
Sparsification methods only transmit the important gradients that
govern the convergence rate of DNN training in each round of parame-
ter synchronization. In general, when gradients are used as the content
of communication, the communication sparsification method selects
part of these elements for transmission, and the gradient elements that
are not selected are replaced with zero values during the decompression
process. Quantization methods involve discretizing continuous gradient
values and mapping them to specified values within a low precision
range. These specified values are represented using fewer bits than the
original values, which reduces the size of each gradient instead of the
number of gradients. During transmission, the quantized values can be
represented with fewer bits, resulting in a decreased communication
overhead and shorter gradient synchronization time.
1.1. Motivations
As the most effective way of alleviating communication burdens in
distributed deep learning, the compression of local models has been
extensively studied in recent years. A few comprehensive surveys [10â€“
12] convey the recent progresses on communication-efficient DNN
training, yet their coverage is too broad to examine the communication
compression in details. An interesting survey [13] is also dedicated
to communication compression that carefully describes the impor-
tant compression algorithms. However, it only classifies the existing
algorithms at the algorithmic perspective. The design rationales of com-
pression approaches adapted to various applications or environments
are not included. In addition, the comparison of different compression
approaches in their convergence rates is not emphasized in the previous
surveys.
Our survey focuses on communication compression in the dis-
tributed DNN training with data parallelism. This survey collects the
current mainstream communication compression techniques and de-
scribes them in detail. We present a taxonomy of the state-of-the-art
that consists of four types of studies: sparsification, quantization, hybrid
compression and context specific compression. We discuss various
approaches for sparsification, including relative value-based, absolute
value-based, and matrix decomposition-based selection methods. For
quantization, we cover truncated expression and code-book mapping-
based selection methods. The hybrid approaches refer to combination
of sparsification and quantization. The context specific approaches
integrate application scenarios and general compression frameworks.
We further explore the differences of these methods considering factors
such as the layer-wise versus global compression, unidirectional versus
bidirectional transmission, i.i.d versus non-i.i.d data distribution, extra
computational overhead, and compare the theoretic convergence rate
and training performance of the selected algorithms. Our survey care-
fully examines a number of new algorithms, and incorporate a set of
new scenarios. In addition, we move one step further to compare the
advantages and disadvantages of versatile compression techniques.
1.2. Paper organization
The remainder of the survey is organized as follows. Section 2 offers
a brief introduction to communication architecture, communication re-
duction strategies and communication compression. Section 3 provides
a taxonomy of communication compression. Sections 4, 5, 6 and 7
review the compression methods in detail. The comparison of different
methods is included in Section 8. We conclude the survey in Section 9.
2. Preliminary
In this section, we introduce the communication architecture of
distributed deep learning. The classical communication reduction tech-
niques are briefly described as the appetizer.
2.1. Distributed deep learning
Distributed deep learning is commonly classified as the Parameter
Server (PS) architecture, the All-Reduce architecture and the decentral-
ized architecture. They mainly differ in the ways that computing nodes
are interconnected and in the approaches that the global consensual
model is reached. As a consequence, the design rationales in these
distributed learning architectures are different.


--- Page 3 ---
Journal of Systems Architecture 142 (2023) 102927
3
Z. Wang et al.
Fig. 2. All-Reduce and Ring All-Reduce.
In the PS architecture shown in Fig. 1, the computing nodes are
tagged with two roles, the worker and the PS. Each worker uses a
subset of training data to generate his local DNN model. The parameter
server is responsible for collecting all the local gradients from the
workers and performing the aggregation. After the aggregated gradient
is generated, each worker pulls it back for model update and starts
the next-round model training. This procedure repeats many rounds
until the global model converges. The PS architecture is throttled by
the communication bottleneck connecting the server and the workers.
The uplink bandwidth is shared by all the workers, so is the downlink.
In light of gigantic sizes of DNN models, the communication time is
commensurate to the local computing time, and may even overwhelm
the latter so that the advantage of distributed training vanishes. As the
computing capability of hardware (e.g. GPU, FPGA and ASIC) improves,
the communication efficiency of the PS architecture degrades accord-
ingly. In addition, when the workers are geographically distributed
and connected through public Internet links, the limited bandwidth
prolongs the wall-clock training time. To alleviate the communication
burden, distributed parameter servers are adopted so that each PS
only processes a fraction of the parameters or gradients, and all the
parameter servers jointly achieves the goal of global aggregation.
All-Reduce is an operation that reduces the target arrays in all
processes to a single array and returns the resultant array to all pro-
cesses. In distributed deep learning, the All-Reduce architecture shown
in Fig. 2(a) enforces the gradient aggregation and the parameter syn-
chronization at every worker in the absence of a central server. It
implements the communication process of gradients in versatile forms,
in which the Ring All-Reduce is the most notable one. The Ring All-
Reduce architecture resembles a directed ring topology as shown in
Fig. 2(b). The local gradient at a worker is sliced into multiple seg-
ments, with each slot allowing only one segment to be delivered from
a worker to his neighbor. This Ring All-Reduce procedure consists of
the â€˜â€˜Scatter-Reduceâ€™â€™ and the â€˜â€˜All-Gatherâ€™â€™ stages. In the first stage,
each worker aggregates the received segment with his own and relays
it to his neighbor step by step until every worker obtains a complete
aggregated segment. In the second stage, each worker sends the fully
aggregated segment to his neighbor via multiple rounds so that the
global consensual model is reached. The All-Reduce architecture has
the potential of avoiding single-link bottleneck, while coordinating the
sequential transmissions is cumbersome especially when the number of
workers is large.
Both the PS and the All-Reduce architectures require each worker to
obtain the global model at every iteration. Their major difference lies in
whether the aggregation is performed at the parameter server or at the
distributed workers. In practice, a worker may communicate only with
its neighbors, thus forming a general decentralized topology. Instead
of obtaining a global model, each worker aggregates the local modes
from its neighbors to yield a partial model. The partial aggregation
between two workers can be bidirectional as shown in Fig. 3(a) or
unidirectional as shown in Fig. 3(b). Compared with PS and All-Reduce,
Fig. 3. Communication based on a graph topology.
the decentralized learning greatly reduces the communication load
since the global model does not go through every worker. In exchange,
the lack of the global consensus prolongs the number of training rounds
to reach the predetermined accuracy.
2.2. Communication reduction
Reducing the number of communication rounds is an effective
approach to cut off the communication overhead in distributed deep
learning. As the basic ideas, one can choose a large batch size to accel-
erate the convergence rate, or to employ the periodical communication
after multiple rounds of local computations.
2.2.1. Large batch training
Batch size is an important hyperparameter in the training of DNN
models that refers to the number of fed data samples at an iteration.
A large batch size yields a more accurate estimation of the gradient
on the entire dataset, thus potentially reducing the number of training
rounds toward convergence. Fewer training rounds mean fewer rounds
of model update. There are drawbacks to choosing a large batch size.
Because DNN models are usually non-convex, the large-batch training
is more inclined to being trapped at a sharp local minimum with large
surrounding curvatures [14]. When the gaps between the local minima
and the global minimum are large, the performance of DNN models will
be hardly satisfactory. Many studies [14â€“16] have confirmed that the
large batch training reduces the model generalizability. Ma et al. [17]
experimentally showed that when the batch size exceeds a certain
threshold, the number of iterations required for the model to eventually
converge increases on the contrary.
2.2.2. Periodic communication
Periodic communication is a commonly used method to reduce the
training traffic, in which multiple local iterations are performed at a
node before gradient synchronization. The reduction of communication
is proportional to the number of local iterations. The costs of peri-
odic communication are the prolonged training rounds and the loss
of final accuracy. When the number of local iterations increases, the
local model may deviate from the global model considerably, so that
the convergence becomes slower in terms of the number of training
rounds to reach a certain level of accuracy. More severely, too many
local updates also drive the model toward the local optimum, thus
impairing the model accuracy and its generalizability [18]. Such a
delicate balance should be carefully treated through the tuning of the
number of local iterations.
2.3. Communication compression
Beyond large-batch training and periodic communication, there
exists an intriguing method, namely communication compression, that
reduces the amount of gradients to be transmitted or encodes a gradient


--- Page 4 ---
Journal of Systems Architecture 142 (2023) 102927
4
Z. Wang et al.
Fig. 4. The hierarchical taxonomy of communication compression techniques.
with less bits at each round. The transmission time of a compressed
model is then shorter. The underlying reason of communication com-
pression lies in that the distributed DNN training is robust to impre-
cise gradients. Consider communication compression in the parameter
server architecture as a use case. When the received local models are
imprecise, the aggregated model may deviate from the true global
model, thus leading to the increased number of iterations to reach
the predefined accuracy. However, the wall-clock time of each itera-
tion is significantly reduced so that the total training time decreases
considerably.
The disadvantage of communication compression is the imprecision
of the local and resultant global models, whereas such imprecision can
be mitigated through compensation techniques. The error accumulation
compensation stores the current compression error locally and compen-
sates for it in subsequent iterations. It ensures that information is not
lost, but only delayed. The earliest error accumulation compensation
method is inspired by Sigma-Delta modulation [19], where the errors
between the genuine and the compressed gradients are cached locally.
These errors are added to the local gradients computed at the next
iteration, and the refined local gradients are compressed subsequently
before transmission. After multiple iterations, the small gradients will
be included in the model training so as to calibrate the deviation
of the global model with communication compression. The gradient
compression as well as the error compensation techniques are indepen-
dent of communication architectures, i.e. deployable in both parameter
server and All-Reduce, and scalable to versatile learning algorithms.
New error accumulation compensation methods have been proposed
currently [20,21].
The simple error compensation technique is not the optimal choice
for variance-reduction optimization algorithms (e.g., Momentum,
Stochastic Recursive Momentum and Implicit Gradient Transport, etc.).
In fact, its implementation may potentially hinder the convergence
speed of Federeated Learning [20]. ErrorCompensatedX [20] (ECX)
uses the compression error of the first two iterations for compensation.
It deployed gradient compression on both the server and the worker
side in the PS architecture. ErrorCompensatedX borrows the idea of a
low-pass filter. The compensation is performed after the compression
error from the previous two iterations is adjusted through a local
scaling factor and the low-pass filter.
3. Overview of compression taxonomy
According to the purpose of compression, we can classify the
communication compression techniques into four major categories
including sparsification, quantization, hybrid compression and context
specific compression in Fig. 4. The former three categories focus on
different ways to squeeze the size of gradients. Sparsification reduces
the number of transmitted elements. Quantization decreases the bit-
width of each element. Hybrid compression reduces both the number
of transmitted elements and the bit-width of each element. On the other
hand, context specific compression takes a different approach, as it pri-
oritizes exploiting the efficiency of the compression techniques within
a given environment rather than reducing communication volume. This
makes context specific compression applicable to most compression
algorithms.
To achieve effective communication sparsification, the selection
method of gradient elements plays a crucial role. This selection de-
termines the sparsity level of the gradients that contributes differently
to model updates. According to the selection strategies, communi-
cation sparsification can be further classified into three categories:
probabilistic sparsification, magnitude sparsification and matrix de-
composition sparsification. In probabilistic sparsification, each element
has the opportunity to be selected for communication. This technique
involves assigning a series of identical or different probabilities to
each element, and then selecting the elements randomly based on their
assigned probabilities. Magnitude sparsification uses a hard metric that
specifies the criteria for elements that can be transmitted. It selects
the gradient elements with top large magnitude for communication
based on the observation that gradient elements with larger absolute
values are more important to the model update. To be more specific,
there are several ways to filter these elements, each with varying
levels of complexity and operational difficulty. The first method is to
use a sorting algorithm. The second method involves comparing each
element with a threshold value to determine if it should be transmitted.
The third method involves using a sketch data structure to identify the
top elements. The selection is carried out with a holistic perspective
in matrix decomposition sparsification. The original large matrix is
approximately decomposed into several matrices of small size. These
small matrices are transmitted among workers.
The essence of communication quantization is to reduce the bit-
width of each gradient element. Two main techniques used to achieve
this goal are truncated expression and code-book mapping. Truncated
expression directly reduces the precision of the element from full
precision to lower precision or even a sign representation. This is
mainly achieved by encoding the elements with fewer bits. Code-book
mapping builds up a code-book and maps continuous element values
onto the discrete quantization values. Only the bit representation of


--- Page 5 ---
Journal of Systems Architecture 142 (2023) 102927
5
Z. Wang et al.
the quantization level is transmitted. The focus of code-book mapping
is the construction of the code-book, especially the setting of the
quantization values. It determines whether the mapping result can
capture the characteristics of the element distribution. According to
the interval between different quantization values, code-book mapping
can be further subdivided into uniform quantization and non-uniform
quantization.
Considering that the underlying mechanisms of sparsification and
quantization are orthogonal, hybrid compression methods come up
with a combined version. In general, the technique quantifies the
gradient elements after sparsification. Sometimes, it compresses the
quantized elements by sparse encoding due to the sparsity of elements
after quantization.
Context specific compression refers to the adaptation of communi-
cation compression in various application scenarios. Communication
architecture and resources are the two factors that have the greatest
impact on compression technology in practical applications. We can
divide context specific compression into two categories according to
these two factors. One category is decentralized compression frame-
works. Decentralized learning has a different communication pattern
where parameters are transmitted between connected neighbors instead
of aggregated by the parameter server. This may incur additional
compression error which will cause learning divergence. Therefore,
decentralized communication compression design has its own unique
characteristics to alleviate this issue. Another category is the envi-
ronment dependent compression framework. In the real-world, the
computation and communication resources are not balanced among
workers and the network environment is always dynamic. Environment
dependent compression frameworks are specially designed to improve
the efficiency of compression methods according to the variability or
imbalance of resources.
4. Communication sparsification
4.1. Communication sparsification basics
The key idea of communication sparsification is to transmit only
large gradient elements that have great impact on the convergence
of DNN model training. At each iteration, a worker typically selects
large elements from the gradient tensor, and transmits two condensed
vectors to the parameter server. One vector contains the values of the
selected gradient elements, and the other corresponds to their indices.
An illustration of the sparsified gradient is shown in Fig. 5. The index
vector can be deemed as the cost of gradient compression because it is
hard to be further compressed. After the parameter server receives the
compressed gradient vector, it is reconstructed into a complete tensor.
The element values whose indices do not appear in the index vector are
filled with zeros.
4.2. Probabilistic sparsification
Probabilistic sparsification filters out the gradient elements in the
communication set according to certain random criteria. The prob-
ability of each element being selected can be identical, or different
depending on its value for the purpose of faster convergence.
Random-k [22] randomly selects ğ‘˜gradient elements out of a total
of ğ‘› elements. It possesses two important properties, i.e. unbiasedness
and low operational overhead. The expectation of the sparsified gradi-
ent after tensor reconstruction is exactly the real gradient. Meanwhile,
Random-k does not involve the sorting of gradient elements, which
yields very light computational complexity. Block-Random-k [23]
selects the first gradient element randomly and then the subsequent(ğ‘˜âˆ’
1) elements immediately. Compared with Random-k, Block-Random-
k further reduces the compression overhead by performing only one
random data access.
Fig. 5. Gradient sparsification.
Random-k ensures the unbiasedness of the gradient while magni-
fying its variance. In order to strike the balance between sparsity and
variance, GradSparse [24] introduces a new probabilistic sparsification
method. Given a fixed variance budget, it builds a convex optimiza-
tion problem to decide the probability of selecting each element in a
gradient vector. One important finding of GradSparse is that the top
elements (i.e. top- ğ‘˜) cannot be dropped. The probability of choosing
each element is given by
ğ‘ğ‘– =
â§
âª
â¨
âªâ©
1, ğ‘– âˆˆ ğ‘†ğ‘¡ğ‘œğ‘ğ‘˜
(ğœ–
ğ‘‘âˆ‘
ğ‘—=1
ğ‘”2
(ğ‘—) +
ğ‘‘âˆ‘
ğ‘—=ğ‘˜+1
ğ‘”2
(ğ‘—))âˆ’1|ğ‘”ğ‘–|(
ğ‘‘âˆ‘
ğ‘—=ğ‘˜+1
|ğ‘”(ğ‘—)|), ğ‘– âˆ‰ ğ‘†ğ‘¡ğ‘œğ‘ğ‘˜.
Here, ğ‘” refers to the original gradient with ğ‘‘ elements and ğ‘”(ğ‘—) is the
ğ‘—th gradient element after sorting. ğ‘†ğ‘¡ğ‘œğ‘ğ‘˜ is the set of coordinates with
top-ğ‘˜ gradient magnitudes. ğœ– is the variance budget. The probability
is approximated by a greedy algorithm to mitigate the computational
overhead of sorting.
Probcomp-LPAC [25] also randomly selects gradient elements for
transmission without sorting them. It first computes the maximum
absolute value in a gradient tensor. Then, the probability of selecting
an element depends on the difference between these two values,
ğ‘ğ‘– = |ğ‘”ğ‘–|
max(|max(ğ‘”)|,|min(ğ‘”)|) .
The smaller the difference, the higher the selection probability. As
an advantage, Probcomp-LPAC has light computational overhead. In
addition, Probcomp-LPAC sets different compression ratios on different
neural network layers. The compression ratio ğ‘ğ‘™ of layer ğ‘™ is given by
ğ‘ğ‘™ = [ğ‘Ÿ+ (ğ‘Ã— ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥3 + ğ‘Ã— ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥2 + ğ‘‘Ã— ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥)] Ã—ğ‘.
Here, ğ‘Ÿ âˆˆ [0 .1] is the initial change rate, and ğ‘, ğ‘ and ğ‘‘ are hy-
perparameters to control the growth of the change rate. The symbol
ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ is marked as the descending order of layer ğ‘™ after being sorted
according to the layer size. With the layer-size dependent compression
ratios, Probcomp-LPAC compresses the layers with more parameters
aggressively and those with less parameters conservatively.
4.3. Magnitude sparsification
Discarding gradient elements with smaller absolute values has less
impact on the convergence rate than discarding larger ones [26].
The reason is simple, i.e. the gradient elements with larger absolute
values contribute more in the decent direction toward the optimum. A
natural way of communication compression is to filter out the gradient
elements based on their magnitudes. We hereby describe two set of
approaches depending on whether the gradient elements are sorted or
not.


--- Page 6 ---
Journal of Systems Architecture 142 (2023) 102927
6
Z. Wang et al.
4.3.1. Sorting and filtering
Top-k [22] selects ğ‘˜ elements with the largest absolute values as
the communication set via a sorting algorithm. The error accumulation
compensation is adopted to combat the biased compression of Top-
k. The unselected small elements are not discarded, but delayed for
transmission in future iterations. Top-k with error compensation is akin
to asynchronous SGD in terms of the delayed transmission, while in
fact they differ critically. In Top-k with error compensation, only a
fraction of gradient elements is delayed and the maximum delay can
be infinite. In asynchronous SGD, all the elements among the same
gradient have the same delay, and the delay is usually assumed to be
bounded. One potential drawback of Top-k is the high compression
overhead, i.e. the time complexity of sorting is nontrivial when the
communication bandwidth is high.
Top-k sparsification gives rise to many extensions. Barnes et al.
presented rTop-k [27], a combination of Top-k and Random-k, which
first selects the largest ğ‘Ÿ (ğ‘Ÿ > ğ‘˜) elements, and then randomly selects
ğ‘˜ elements in these elements as the communication set for trans-
mission. Note that the design of rTop-k is rooted in the distributed
statistical estimation theory. A sparse Bernoulli distribution is used to
characterize the distribution of gradient values. The larger values are
observed as â€˜1â€™ and the smaller values are observed as â€˜0â€™. For the
lower bound on estimation error under sparse Bernoulli distribution,
this work compares the result of random subsampling with that of
arbitrary estimators. It shows that rTop-k is optimal in terms of order
up to logarithmic factors.
Both vanilla Top-k and rTop-k transmit the sparsified gradient to
the parameter server, while leaving the downloading process of the
aggregated model unaltered. In gTop-k [28], not only the workers
execute the Top-k sparsification scheme, but also the parameter server
uses it over the aggregated gradient and sends back the sparsified
one. The advantage of gTop-k is the significant reduction of downlink
traffic load. Because the indices of the top- ğ‘˜ elements computed by
each worker vary greatly, after averaging the number of gradient
elements scales up by a factor of the number of workers. With the
Top-k sparsification at the parameter server, the downlink bottleneck
is mitigated.
Scalecom [29] uses the local gradient similarity of different workers
to perform a â€˜â€˜globalâ€™â€™ Top-k algorithm. When the number of workers
becomes large, two problems arise. One is that the downlink com-
munication becomes the bottleneck, the other is the scaled learning
rate required for large-scale training increases the gradient noise sig-
nificantly. To reduce the downlink traffic, Scalecom enforces all the
workers to select the gradient elements of the same indices. It intro-
duced the Cyclic Local Top-k (CLT-k) algorithm so that the indices of
the global model coincide with those of one leading worker. As an
additional advantage, Scalecom is adaptable to the pipelined transmis-
sion of the Ring All-Reduce architecture. To tackle the second problem,
ScaleCom designed a low-pass filter that reduces the noise amplification
caused by the scaled learning rate, and increases the gradient similarity
across different workers.
LGC [30] asserts that the gradients between workers are corre-
lated, and it exploits the redundancy between workers to improve the
compression efficiency. LGC divides layer gradients of each worker
into the public component and the innovation component. The public
component shared by all the workers can be eliminated and only the
innovation component unique to each worker needs to be captured. The
innovation component is the top-magnitude gradients.
LGC designs the corresponding autoencoders realized with
lightweight neural networks for the Parameter Server architecture
and the Ring All-Reduce architecture, respectively. In the Parameter
Server architecture with K workers, it requires one encoder and K
decoders. The elements from all the workers are sequentially passed
to the encoder for encoding with the goal of minimizing the similarity
loss. The corresponding decoder reconstructs the gradients with the
objective of minimizing reconstruction loss based on the public and
innovation component. In the Ring All-Reduce architecture, only one
encoder and one decoder are required. The elements in the innovation
component are in the same position for each worker. And the rest
of the operations are similar to the operations in Parameter Server
architecture.
4.3.2. Threshold comparison
An alternative sparsification approach is to use a threshold value to
filter out the unimportant gradient elements, thus avoiding the cum-
bersome sorting operation. In contrast to the sorting-based algorithms,
the threshold-based ones need to select an appropriate threshold value,
and lose the exact control of the compression ratio.
Threshold-ğ‘£[31] uses two thresholds, a positive one and a negative
one to filter out the gradient elements. Only those above the positive
threshold and lower than the negative threshold are selected as the
communication set. One clear benefit of Threshold- ğ‘£ is the low com-
putational overhead, and its drawback is the inflexible choice of the
thresholds. Deep neural networks differ in their network structures and
optimizers so that their optimal thresholds are not coincident.
DGC [32] is a threshold based sparsification approach, although
it implements the Top-k selection. At the first step, DGC randomly
samples a small fraction of gradient elements, e.g. 0.1% to 1% of
the total. The sparsification threshold is obtained by applying a Top-
k algorithm on this sampled gradient. This threshold is further used to
select large elements in the original gradient. The advantage of DGC is
its reduction on the computational complexity of Top-k significantly.
If the number of elements filtered out is much higher than expected,
an exact threshold is then calculated from the filtered elements. To
compensate for the loss of accuracy and gradient staleness due to
compression, auxiliary techniques are incorporated such as error accu-
mulation compensation, momentum correction, local gradient clipping,
momentum factor masking and warm-up training.
For the momentum SGD, the current DGC with error feedback is
not directly applicable. Sparse updates will omit the discounting factor
term in model update, changing the optimization direction [33]. In
order to correct the direction, momentum correction accumulates the
new gradient with momentum instead of the actual gradient. To avoid
gradient explosion, local gradient clipping is performed. This method
rescales the gradients when the sum of their L2-norms exceeds a certain
threshold. Momentum factor masking aims to prevent stale momentum
from deflecting optimization. The momentum factors associated with
the unsent cumulative gradients are set to zero. In the early stage of
training, sparse updates can lengthen the period of neural network
changes dramatically. Warm-up training uses a small learning rate and
a less aggressive sparsification at the start of training to reduce the
impact of early gradients.
GradDrop [34] approximates the threshold by sampling estimation,
ensuring that there are enough elements larger than this threshold.
It designs the layer-wise thresholds or global threshold for the entire
model. Since the difference in gradients across layers can be large,
the calculation of global threshold needs to be combined with layer
normalization. Without layer normalization, the global threshold may
degrade model convergence.
RedSync [35] proposes two efficient implementations of Top-k
on GPUs via threshold comparison, where one is the trimmed top- ğ‘˜
selection and the other is the threshold binary search selection. In the
first method, the initial layer-wise threshold is calculated based on the
ratio, the mean and the maximum values of each tensor layer, i.e.,
ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ = ğ‘šğ‘’ğ‘ğ‘›+ ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œÃ— (ğ‘šğ‘ğ‘¥âˆ’ ğ‘šğ‘’ğ‘ğ‘›).
If the number of elements filtered by the threshold is smaller than the
expected number, it dynamically decreases the threshold so that more
elements can be filtered. The RadixSelect algorithm is performed on the
remaining elements of a relatively smaller subset to obtain the accurate
sparsified result. The second method searches for the threshold between
the first ğ‘˜ and 2ğ‘˜ largest elements (in terms of their magnitudes) via


--- Page 7 ---
Journal of Systems Architecture 142 (2023) 102927
7
Z. Wang et al.
the canonical binary search. The compression time of RedSync is non-
negligible when the size of a layer is large. To reduce the computational
overhead, RedSync treats different layer sizes dissimilarly. For a small
layer, the top- ğ‘˜ method is used directly; for a medium sized layer,
the trimmed top-ğ‘˜ method is used and for a large layer, the threshold
binary search selection is adopted.
MSTop-k [36] considers communication sparsification in a large
cluster with multiple machines and multiple cards. This method aims
to address the inefficiency of top- ğ‘˜ selection on GPUs and the expen-
sive communication overhead of All-Gather. It implements the top- ğ‘˜
selection by multiple samplings. MSTop-k first uses the mean value
as the first threshold, and then halve or double it as the second
threshold. After that, the process is repeated continuously and these
two thresholds are updated until the required number of searches is
reached. Because the commonly used sorting algorithm involves many
irregular memory access, MSTop-k is more efficient on memory care
units such as GPUs.
To reduce the communication overhead of All-Gather in large sys-
tems, MSTop-k further proposes the hierarchical top- ğ‘˜ communication
algorithm HiTopKComm. Each worker performs an intra-node Reduce-
Scatter operation in parallel so that every GPU has a portion of the
local gradients. The MSTop-k selection is used in each GPU to compress
the tensor. The corresponding GPU performs All-Gather operation be-
tween workers. An intra-node All-Gather is conducted to complete the
aggregation. HiTopKComm makes full use of the bandwidth resources
of intra-node and inter-node connections to improve communication.
EGC [37] dynamically determines the sparsification threshold of
each layer based on the entropy of its gradient elements. The intuition
behind this design lies in that a large entropy is inclined to carrying
more important information so that less gradient elements should be
pruned. At each layer, EGC partitions the range of gradient elements
equally into ğ‘ intervals, and the number of gradient elements fell in
each interval is counted subsequently. Let ğ‘ğ‘– be the fraction of the
gradient elements in the ğ‘–th interval, and let ğ» be the entropy of this
layer that has
ğ» = âˆ’
ğ‘âˆ‘
ğ‘–=1
ğ‘ğ‘–ğ‘™ğ‘œğ‘”ğ‘ğ‘–.
The fraction of gradients sent by the worker is set to min{ ğ»
ğ¾ ,1} where
ğ¾ is the predefined constant. Hence, when ğ» is too small, it is rea-
sonable to send less gradient elements, and send more otherwise. The
sparsification threshold can be determined using Quickselect [38].
To compensate for the accuracy loss, EGC incorporates automatic
adjustment of the learning rate that adopts to the change of the training
loss. EGC can be implemented in different distributed architecture. It
proposes the EGC-PS for the parameter server architecture, the EGC-
DSGD for the All-Reduce architecture and the EGC-FL algorithm for
federated learning.
To compensate for the gradient staleness from error accumulation,
GradSA [39] performs sparsification based on historical gradient in-
formation. It uses the result of Taylor expansion of fresh gradients
to replace the original historical gradients and adds a decay factor
to the accumulation. In addition, GradSA also adopts a layer-by-layer
sparsification approach in order to improve the quality of the sparsified
gradients.
Knowing that the standard deviation of gradient elements indicates
their degree of dispersion, SDAGC [40] leverages it to dynamically cal-
culate the sparsification threshold. For a particular layerğ‘™, its threshold
is given by
ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ğ‘™ = ğ›¼Ã— ğœğ‘™,
where ğ›¼ is the hyperparameter to control gradient sparsity and ğœğ‘™ is
the standard deviation of gradients. The auxiliary approaches including
residual gradient accumulation, local gradient clipping, adaptive learn-
ing rate correction and momentum compensation are used for such an
aggressive sparsification to avoid the decrease of training accuracy.
Gaussian-K [41] models the gradient distribution as a normal
distribution to obtain the threshold estimate. It uses the mean value
and variance of the original gradient to construct a normal distribution.
The percentage point function is used to calculate the threshold. Since
the true gradient distribution does not exactly match the normal distri-
bution, the number of selected elements may deviate from the desired
one. To select the desired number of gradients, Gaussian-K adjusts the
threshold by shifting it left or right.
SIDCo [42] fits the gradient distribution and estimates the threshold
corresponding to the compression ratio based on the fitting results.
It proposes three different distributions, which are double exponen-
tial distribution, double gamma distribution and double generalized
Pareto distribution. Although they fit the true gradient distribution in
the DNN well, there are some deviations in the tail of the CDF. To
address inaccurate estimation due to the deviations, the single-stage
and multi-stage compressions are proposed. For general compression
ratios, SIDCo performs only one fitting to obtain an accurate threshold.
For very aggressive compression ratios, SIDCo turns them into multiple
compression, and uses different distribution models to fit and filter the
gradients.
4.3.3. Sketch mapping
The Sketch data structure uses multiple hash functions to map the
original elements to a contiguous memory space [43]. It supports two
operations, insert and query. Insert operation updates elements in the
sketch and query operation estimates the original elements from the
sketch. Sketch uses limited spatial resources to estimate the overall
information and can achieve a better balance between estimation ac-
curacy and memory usage. As a result, many works make the use of
Sketch for communication compression.
Of many Sketch data structures available, Count Sketch is the most
commonly used in communication compression. It compresses a vector
ğ‘”(ğ‘‘) into a Sketch ğ‘†(ğ‘”) of size ğ‘‚(logğ‘‘). On one hand, Count Sketch can
obtain the heavy hitters in the original elements and use them as an
accurate estimate of the top- ğ‘˜ largest elements. Thus, sketch mapping
is considered a special kind of sparsification. On the other hand, the
linearity of Count Sketch, i.e. ğ‘†(ğ‘”1)+ ğ‘†(ğ‘”2) =ğ‘†(ğ‘”1 +ğ‘”2), makes it widely
adopted in distributed training. A major difference between Sketch and
general sparsification is that the aggregated result does not scale with
the number of workers. As a result, sketch mapping-based sparsification
methods do not congest the downlink communication in large-scale
distributed systems.
Spring et al. compress auxiliary variables in the optimization algo-
rithm based on Count Sketch [44]. It aims to free up the memory for
either a more expressive model or a larger batch training. They empir-
ically find that the auxiliary variables show a power-law distribution
during the training. Only a fraction of the elements makes a significant
contribution to the model update. In addition, these auxiliary variables
are updated in a linear fashion, which is suitable for Sketch. Therefore,
the Count Sketch can be used for the compression of these auxiliary
variables.
Spring et al. only compress for auxiliary variables and do not
endeavor to reduce the communication overhead. Sketched-SGD [45]
uses Count Sketch to compress gradients in communication and updates
the model with global top- ğ‘˜ gradients. It requires two rounds of com-
munication between the workers and the parameter server. In the first
round, each worker transmits the sketch results of the local gradients.
The parameter server aggregates these sketches and recovers the indices
of the global top- ğ‘˜ elements through the Heavy Mix algorithm. In the
second round, the parameter server collects the corresponding elements
from all workers for gradient aggregation. To improve the conver-
gence speed, Sketched-SGD also employs gradient error accumulation
compensation, momentum correction and momentum factor masking
techniques similar to DGC.
FetchSGD [46] makes use of Count Sketch for communication
compression in Federated Learning. In Federated Learning, there are


--- Page 8 ---
Journal of Systems Architecture 142 (2023) 102927
8
Z. Wang et al.
problems with client drop, in addition to inefficient communication.
Sketched-SGD requires two rounds of communication to complete an
iteration. However, a client may participate only once during all of
training. The momentum and error information on the clients is easily
lost. Therefore, FetchSGD recovers the top-ğ‘˜elements directly with the
merge of sketches and completes the momentum and error accumula-
tion compensation on the aggregator.
4.4. Matrix decomposition sparsification
The gradient elements of a DNN layer can be viewed as a matrix,
where the number of columns corresponds to the input neurons and
the number of rows corresponds to the output neurons. Using singular
value decomposition (SVD), a matrix can be expressed as a product of
three matrices where the singular value matrix is in the middle. If we
retain a subset of large singular values, the corresponding columns of
the first matrix as well as the corresponding rows of the third matrix,
the original matrix can be constructed approximately. Therefore, in-
stead of transmitting the full gradient tensors, matrix decomposition
only sends the selected values used for gradient reconstruction, thus
greatly reducing the traffic volume. Similar to above mentioned spar-
sification approaches, matrix decomposition transmits less elements in
the important basis vectors.
The advantages and disadvantages of matrix decomposition sparsi-
fication are very intuitive. First, in contrast to ordinary sparsification
methods, it does not designate the indices of selected elements. There-
fore, it is naturally integrated in the All-Reduce architecture without
any compression coordination. Second, matrix decomposition sparsi-
fication does not require the carefully designed error compensation
techniques to ensure model convergence [47]. Third, the use of low-
rank update can be treated as the spectral regularization [48], which
facilitates the model generalizability [49]. However, this matrix decom-
position has some criticisms that the computational overhead is huge
and the compression ratio can be set arbitrarily.
GradiVeQ [50] compresses the gradient matrix ğ‘” with the whiten-
ing vector ğœ‡ğ‘š and the matrix ğ‘ˆğ‘‘,ğ‘š. ğœ‡ğ‘š is the average result of the
gradient matrix in the previous ğ¿ğ‘¡ uncompressed iterations. ğ‘ˆğ‘‘,ğ‘š is the
matrix consisting of the firstğ‘‘columns of the eigen matriceğ‘ˆğ‘š. And ğ‘ˆğ‘š
is obtained from the covariance matrix calculated from the ğ¿ğ‘¡ samples
by SVD. GradiVeQ compresses the original gradient ğ‘”into ğ‘ˆâŠ¤
ğ‘‘,ğ‘š(ğ‘”âˆ’ğœ‡ğ‘š).
PowerSGD [51] uses two low-rank matrices ğ‘ƒ and ğ‘„ to approxi-
mate the original gradient matrix byğ‘ƒğ‘„ğ‘‡. The computation of ğ‘ƒ and ğ‘„
is based on one power iteration and the Grammâ€“Schmidt orthogonaliza-
tion. In addition, the authors extend it to decentralized scenarios [52].
The low-rank decomposition with power iterations is used to compress
the model differences between neighboring nodes.
4.5. Summary and comparison
In this subsection, we summarize and compare aforementioned
sparsification algorithms. Generally, a sparsification algorithm develops
a dedicated rule to directly or indirectly filter the gradient elements and
transmit them together with their corresponding indices. The difference
from the compressed gradient and the original one should be as small
as possible in terms of their directions and magnitudes. As the rule of
thumb, the large elements in a gradient are retained while the smaller
elements are discarded in most of the sparsification algorithms. The
major differences among them are summarized in four aspects.
(1) Element-wise sparsification versus holistic sparsification.
The mainstream sparsification algorithms belong to the element-
wise genre. They treat a gradient element as an atomic unit, and merely
select a subset of gradient elements without modifying their values.
The representative element-wise algorithms include Random-k, Top-k,
DGC, etc. The holistic sparsification is featured with two approaches,
sketch mapping and matrix decomposition. The sketch mapping makes
use of a sketch data structure to transform the gradient elements
into corresponding hash values, and hence the quantity of elements
can be greatly reduced. The compression ratio of sketch mapping is
subject to the size of the sketch data structure, but it can recover
the global top- ğ‘˜ gradient elements due to its linearity. Singular value
decomposition is used to approximate the matrix-form gradient with a
set of condensed matrices. Once being received by the server, they can
recover all the gradient elements with gentle errors. The computational
overhead of holistic algorithms are much larger than their element-wise
counterparts.
(2) Probabilistic sparsification versus deterministic sparsification.
The probabilistic methods, as the name indicates, select gradi-
ent elements randomly, and representative ones include Random-k,
GradSparse and Probcomp-LAPC. In particular, the latter two meth-
ods assign larger probabilities to the gradient elements with large
magnitudes. The deterministic sparsification methods usually choose
a threshold for pruning the gradient elements, or even sort them at
a descending order. For example, DGC, threshold- ğ‘£, Top-k and rTop-k
all belong to the deterministic sparsification. The probabilistic methods
demand the minimum computational complexity in contrast to the
deterministic methods. These two types of methods are usually com-
bined to balance the computational complexity and the precision of the
sparsified gradient.
(3) Treatment on residuals.
The difference between the sparsified and the original gradients is
called the residual. Although the residual is nonessential to the model
update in one iteration, its accumulation over multiple iterations may
influence the direction and magnitude of the gradient considerably.
Simply discarding the residual impairs the convergence speed, and
even cannot guarantee the exact convergence toward optimality under
canonical strongly convex and smooth conditions in theory. To cope
with this challenge, error accumulation compensation is presented for
gradient sparsification, especially when the sparsity ratio is high. Most
of the up-to-date algorithms tacitly approve this technique.
(4) Complexity order of compression.
We hereby only refer to the computational complexity that pos-
sibly prolongs the wall-clock time of an iteration, especially when
the bandwidth is large and the compression ratio is high. Nominally,
probabilistic sparsification algorithms such as Random-k have the low-
est complexity order, i.e. îˆ»(ğ‘˜), that is independent of the gradient
size. However, generating a random index is time-consuming such that
Random-k is not a de facto efficient approach. The sorting algorithms,
featured by Top-k, incur a much higher complexity order îˆ»(ğ‘‘log ğ‘˜)
on average using Quick Sort (despite of numerous sorting algorithms,
Quick Sort is taken as the baseline) in a gradient with ğ‘‘ elements. The
threshold algorithms is of moderate complexity order that usually takes
the form îˆ»(ğ‘‘). The actual computation time is pertinent to the threshold
estimation that constitutes their core contribution and determines the
running time well. The computation overheads of sketch mapping
and matrix decomposition are much higher than the aforementioned
approaches.
We summarize our comparisons in Table 1.
5. Communication quantization
Communication quantization reduces communication overhead
with a lower accuracy to express each element. The bit width to
represent each element is shortened. The communication quantization
methods can be divided into two types. The first type is truncated
expression that directly reduces the number of bits for each element
from 32 bits to 16 bits, 8 bits or even 1 bit, as shown in Fig. 6(a).
The second type is Code-Book mapping that turns continuous element
values into certain quantization levels with corresponding discrete
values.
The Code-Book mapping method typically involves deflating the
original element values to a range between 0 and 1 before mapping.
During communications, each worker transmits a sign vector, a scaling


--- Page 9 ---
Journal of Systems Architecture 142 (2023) 102927
9
Z. Wang et al.
Table 1
Comparison of different sparsification algorithms.
Algorithm Element-wise or Holistic Probabilistic or Deterministic Error compensation Complexity order of compression
Random-k Element-wise Probabilistic âœ“ îˆ»(ğ‘˜)
GradSparse Element-wise Probabilistic Ã— Greedy algorithm
Probcomp-LAPC Element-wise Probabilistic Ã— îˆ»(ğ‘‘)
Top-k Element-wise Deterministic âœ“ îˆ»(ğ‘‘log ğ‘˜)
rTop-k Element-wise Deterministic âœ“ îˆ»(ğ‘‘log ğ‘˜)
gTop-k Element-wise Deterministic âœ“ îˆ»(ğ‘‘log ğ‘˜)
ScaleCom Element-wise Deterministic âœ“ îˆ»(ğ‘‘log ğ‘˜)
LGC Element-wise Deterministic âœ“ îˆ»(ğ‘‘log ğ‘˜)
Threshold-v Element-wise Deterministic Ã— îˆ»(ğ‘‘)
GradDrop Element-wise Deterministic âœ“ îˆ»(ğ‘‘)
DGC Element-wise Deterministic âœ“ îˆ»(ğ‘‘)
RedSync Element-wise Deterministic âœ“ îˆ»(ğ‘™ğ‘œğ‘”ğ‘‘ + ğ‘‘)
MSTop-k Element-wise Deterministic Ã— îˆ»(ğ‘‘)
EGC Element-wise Deterministic âœ“ îˆ»(ğ‘‘)
SDAGC Element-wise Deterministic âœ“ îˆ»(ğ‘‘)
Gaussian-k Element-wise Deterministic âœ“ îˆ»(ğ‘‘)
SIDCo Element-wise Deterministic âœ“ îˆ»(ğ‘‘)
Sketched-SGD Holistic Deterministic âœ“ îˆ»(ğ‘‘)
FetchSGD Holistic Deterministic âœ“ îˆ»(ğ‘‘)
GradiVeQ Holistic Deterministic Ã— îˆ»(ğ‘‘3)
PowerSGD Holistic Deterministic Ã— Power iteration
Fig. 6. Gradient quantization.
factor and a vector consisting of many quantization levels with a
few bits, as shown in Fig. 6(b). It is worth noting that quantization
methods cannot achieve a very high compression ratio because each
element consumes at least one bit. If each original element takes 32
bits, quantization methods can achieve the highest compression ratio
of 1
32 .
5.1. Truncated expressions
Various truncated expressions mainly differ in how many bits are
used to represent a gradient element. FP16 [53] and 8-BIT APPROXI-
MATIONS [54] use 16 bits and 8 bits to represent each element, respec-
tively. The extreme cases of truncated expression are 1-Bit SGD [55]
and SignSGD [56], both of which adopt a single bit to represent each
element.
FP16 [53] directly changes the precision of gradient elements from
full-precision (float32) to half-precision (float16) by type-casting before
transmission. The range of float32 is much different from that of
float16, which might lead to numerical overflow. To avoid overflowing,
FP16 reduces the range of gradient values by loss scaling in each
iteration, and then the same scaling factor will be used to recover
gradient elements.
8-BIT APPROXIMATIONS [54] uses 8 bits to represent each el-
ement, achieving a 4 Ã— compression. The representation of a floating
point number includes sign part, exponent part and mantissa part. 8-
BIT APPROXIMATIONS adopts a dynamic 8-bit representation without
fixing the number of exponent bits and mantissa bits. It uses a flag bit to
distinguish the exponent part from the mantissa part. Flag bit is the first
â€˜1â€™ after the sign bit. The number of â€˜0â€™ between the sign bit and flag bit
represents the exponent part, and the part after the flag bit represents
the bifurcated dichotomous tree on the interval (0.1, 1). The method
approximates large absolute values with small errors while retaining
the ability to approximate small absolute values.
1-Bit SGD [55] uses a single bit to represent each element. It
encodes elements whose values are less than the defined threshold as
â€˜0â€™ and other elements as â€˜1â€™. In the decompression process, â€˜0â€™ and â€˜1â€™
is reconstructed to the mean value of negative and non-negative local
gradients. To further reduce the quantization error, 1-Bit SGD uses the
error accumulation compensation technique.
SignSGD [56,57] also uses a single bit to represent an element, but
only transmits the sign information. SignSGD quantizes the negative
element to â€˜0â€™ and the non-negative element to â€˜1â€™. In other words, it
recovers quantized elements to either âˆ’1 or 1. SignSGD uses the major-
ity voting to decide the sign of aggregated gradients. This method can
tolerate packet losses during communications and adversarial attacks
with fewer than half of attack workers. SignNUM [56] incorporates
the momentum method into SignSGD. In addition, in order to reduce
quantization error, EFSignSGD [58] combines with the error accumu-
lation compensation technique with SignSGD. It transmits both the sign
information and the value information together with the scaling factor.
LE-SignGD [59] further introduces a delayed aggregation technique
to determine whether to skip the current communication or not. It
defines a per-communication descent, i.e., the ratio of the one-step
descent in SignGD to the total communication traffic in the cluster.
LE-SignGD aims to obtain the largest per-communication descent by
selecting a subset of workers in each communication round. If a lo-
cal gradient is small enough, the corresponding worker will skip the
communication of this gradient.


--- Page 10 ---
Journal of Systems Architecture 142 (2023) 102927
10
Z. Wang et al.
Adam has shown a superior convergence speed than SGD in a
variety of deep learning tasks. However, it is hard to combine the com-
pression and such non-linear gradient-based optimization algorithms
together. Although error compensation is a common and effective
technique in compression, it cannot be directly applied to Adam. There
are two auxiliary variables, the variance term and the momentum term,
in Adam. The compression error from the update of variance term
contains a quadratic term, which cannot be eliminated with training.
Thus, the compressed variance term cannot be estimated accurately.
It is also required in the update of the momentum term to scale the
compression error. To solve these problems, Tang et al. proposed 1-bit
Adam [60] based on the observation that the variance term of Adam
becomes stable with training. 1-bit Adam first trains the model with
the vanilla Adam as a warm-up. The variance term is fixed once it
becomes stable. Then, the one-bit compression with error compensation
(i.e., sign compression) is performed in the momentum term.
5.2. Code-Book mapping
Code-Book mapping can be divided into uniform and non-uniform
methods based on the selection of quantization intervals. The former
has the same interval value between two adjacent quantization levels;
the latter has different interval values, and is more advantageous in
expressing the distribution of gradient elements.
5.2.1. Uniform quantization
QSGD [61] maps continuous elements in ğ‘” via random rounding to
fixed discrete values with the scaling factor pertinent to the original
gradient.
ğ¶ğ‘„ğ‘†ğºğ·(ğ‘”ğ‘–) =â€–ğ‘”â€–2 â‹…ğ‘ ğ‘–ğ‘”ğ‘›(ğ‘”ğ‘–) â‹…ğœ‰ğ‘–(ğ‘”,ğ‘ ).
Here, â€–ğ‘”â€–2 is the L2-norm of the original tensor representing the scaling
factor, and is used for both quantization and dequantization. ğ‘ ğ‘–ğ‘”ğ‘›(ğ‘”ğ‘–) is
the sign function, indicating the sign value of the original element ğ‘”ğ‘–; ğ‘ 
is the total number of quantization levels, indicating that the range (0,
1) is equally sliced into ğ‘ levels; ğœ‰ğ‘–(ğ‘”,ğ‘ ) is a random variable defined by
ğœ‰ğ‘–(ğ‘”,ğ‘ ) =
â§
âª
â¨
âªâ©
ğ‘™
ğ‘  1 âˆ’ğ‘
(ğ‘™+ 1)
ğ‘  ğ‘= ğ‘  |ğ‘”ğ‘–|
â€–ğ‘”â€–2
âˆ’ ğ‘™.
where ğ‘™ (0 â‰¤ ğ‘™ < ğ‘ ) is an integer satisfying the condition of |ğ‘”ğ‘–|
â€–ğ‘”â€–2
âˆˆ [ ğ‘™
ğ‘ , (ğ‘™+1)
ğ‘  ]. QSGD ensures the unbiased quantization by introducing
random variables. ECQ-SGD [62] applies the error accumulation com-
pensation technique to QSGD to further reduce the negative impact of
quantization error.
TernGrad [63] also performs quantization by a scaling factor re-
lated to the original tensor. Unlike QSGD, TernGrad specifies the quan-
tization of all elements to only three levels {âˆ’1,0,1}, and its quantiza-
tion results of tensor ğ‘” can be expressed as
ğ¶ğ‘‡ğ‘’ğ‘Ÿğ‘›ğºğ‘Ÿğ‘ğ‘‘(ğ‘”ğ‘–) =â€–ğ‘”â€–âˆ â‹…ğ‘ ğ‘–ğ‘”ğ‘›(ğ‘”ğ‘–)â—¦ğ‘.
The quantization method is similar to that of QSGD. The difference
is that the scaling factor uses the maximum of the absolute value
(infinite norm â€–ğ‘”â€–âˆ) in the original elements where â—¦ denotes the
Hadamard product. Each element ğ‘ğ‘– in ğ‘ obeys Bernoulli distribution
independently, i.e.,
â§
âª
â¨
âªâ©
ğ‘ƒ(ğ‘ğ‘– = 1|ğ‘”) = |ğ‘”ğ‘–|
â€–ğ‘”â€–âˆ
,
ğ‘ƒ(ğ‘ğ‘– = 0|ğ‘”) = 1 âˆ’|ğ‘”ğ‘–|
â€–ğ‘”â€–âˆ
.
Similarly, TernGrad introduces random variables to ensure quanti-
zation results unbiased. Because TernGrad has only three quantization
levels, the vast majority of the elements will be quantized to 0 when the
scaling factor is too large. Typically, most parameters will be changed
slightly while only a small subset of them is updated significantly,
resulting in a significant training variance. Therefore, TernGrad chooses
the scaling factor layer-by-layer and performs the layer-wise trival-
orization. Gradient clipping method is used to approximate the normal
distribution of elements in order to prohibit the training variance.
In DIANA [64], both the workers and the server maintain a mem-
ory initialized to arbitrary values. Each worker performs a random
trivalorization of the difference between the local gradient and local
memory. The server collects the quantized differences from all the
workers. The completion of the gradient aggregation relies on these
collected differences and the memory in the server. The local memory
in each worker is updated by its quantized difference. The server uses
the aggregation result of all the differences to update the memory.
DIANA can be combined with the Nesterov acceleration [65] and
FISTA [66] in ADIANA [67]. CANITA [68] combines the acceleration
algorithm ANITA [69] with DIANA.
In MARINA [70], each worker performs communication compres-
sion with some probability. Some workers quantize the difference of
local gradients in two consecutive iterations and send it to the server.
The remaining workers transmit the original local gradient directly.
MARINA leads to a biased global gradient estimation. It can help the
model to jump out of the local optimum trap in training.
3LC [71] is a three-valued quantization with lossless quadratic
coding and zero-run coding. In the three-value quantization process,
3LC sets a sparse multiplier for scaling to control the sparsity of
the quantization results. A simple rounding function ğ‘…ğ‘œğ‘¢ğ‘›ğ‘‘() is used
for trivalorization in this process. The biased quantization from the
rounding function needs to be compensated by the error accumulation
compensation. Inspired by the quadratic polynomial expression, 3LC
performs a lossless fixed-length encoding called quadratic encoding in
a group of five trivialized results. In addition, 3LC also performs lossless
zero-run encoding by exploiting the sparsity of the quantization results
to recompress consecutive zero values. Therefore, 3LC can achieve a
very high compression ratio.
FedPC [72] proposes a synchronous training algorithm that can
preserve privacy while improving communication efficiency through
ternary quantization in the workerâ€“master architecture. The dataset,
learning rate, batch size and optimizer used for local training in each
worker are private information, which is invisible to the master and
other workers. In addition, the size of workersâ€™ datasets is heteroge-
neous. FedPC deploys the algorithm on both the master and workers.
At the beginning of each training epoch, all workers train the global
model from the master locally to get a local model instance and an
evaluated cost (e.g., loss function value). Each worker sends its cost to
the master and waits for the masterâ€™s command. The master selects the
best model instances by means of a goodness function. The function
jointly considers the size of a local dataset and the cost from workers.
The selected worker sends the entire model instance to the master, and
the remaining workers trivializes their model instances according to
the change in models before communication. The global model update
is performed by the master after receiving the information from all
workers.
For the goodness function in the first training round, the master
selects instances from workers carrying the lowest cost per data sample.
Since the second training round, the master tends to select instances
from workers with both a large dataset size and a large reductions of
cost compared to the previous training round. After that, the master
sends the command to all workers. For the trivalorization in the first
training, each worker calculates the variation between the local model
instance and the global model randomly initialized by the master. It
is compared with the magnitude of local learning rate. The elements
with large changes are quantified as +1 or âˆ’1. Whereas, the rest are
quantified as 0. Starting from the second training round, each worker
keeps the global model of the first two training rounds locally. The
model instance is quantified based on the degree of the change in two
model updates.


--- Page 11 ---
Journal of Systems Architecture 142 (2023) 102927
11
Z. Wang et al.
It is worth noting that FedPC consumes a certain amount of memory
resource at both master and workers. Each worker needs to save the
global model of the previous two training rounds. The master needs
to store the cost of each worker in the previous training round and
the corresponding size of a local dataset, which puts much pressure
on storage when the number of workers is huge. Besides, due to the
heterogeneity of workersâ€™ private information, they vary in the time
spent in a single training round. There may exist workers which are on
hold for a long time with a low resource utilization.
With 2-bit quantization, CD-SGD [73] designs a special update rule
so that compressed communication and local updates can be carried
out in fully parallel. Each worker updates the model with the gradients
based on its local parameters. At the same time, the local gradients are
quantized and sent to the servers. The communication and compression
overhead can be largely hidden. When receiving the global parameter,
the workers update their local model. ğ¾-step correction is used to
transfer the full gradients everyğ¾ iterations for correcting the direction
of model update.
FedPAQ [18] and PQASGD [74] are composite compression that
combine periodic communication and unbiased quantization to im-
prove training efficiency. After each worker updates its local model
for multiple rounds on the basis of the synchronized global model, it
compresses the change between them for transmission. The parameter
server performs the parameter synchronization after receiving all the
compressed changes, and proceeds to the next periodic communication.
FedPAQ allows the partial participation of workers. During each period,
only a fraction of workers performs model training and communication.
LAQ [75] combines gradient quantization and lazily uploading. The
quantization of the gradients depends on its relation to the previous
quantized gradients. Each gradient element is mapped to the near-
est discrete point in a uniform Code-Book centered on the previous
quantized element with their difference as the radius. There is a rule
for uplink communication in all the workers. Whether or not uplink
communication is performed depends on the difference between the
quantized gradients at current iteration and at the last uplink communi-
cation. The worker pushes the quantized information to the servers only
when the difference is greater than a certain threshold. To prevent the
problems caused by gradient staleness, LAQ sets a maximum skipping
period. If a worker skips the uploading a certain number of times, it
will be forced to participate in the next communication.
ATOMO [76] is a matrix-atomization-based compression method
that represents the gradient matrix ğ‘” as a linear combination of simple
components (atoms) in the inner product space. The compression result
is ğ‘”= âˆ‘
ğ‘âˆˆîˆ­ ğœ†ğ‘ğ‘for some sets of atoms îˆ­ with â€–ğ‘â€– = 1. The objective is
to minimize the variance in the atomic basis and keep it as an unbiased
estimator of the original gradient.
AdaQS [77,78] uses the MSDR value of gradient elements as a
metric to determine the quantization ratio of QSGD. MSDR can reflect
the â€˜â€˜signal-to-noise ratioâ€™â€™ of gradient elements. When MSDR is less
than a certain percentage of its historical value, the quantization level
is doubled to suppress noises. Meanwhile, the historical value of MSDR
is updated accordingly.
Oland et al. dynamically select the number of quantization levels
for each layer by using the root mean square value of the layer
gradients [79]. MQGrad [80] adopts a reinforcement learning based
approach to determine the number of quantization levels by learning
the change of the training loss. However, these two methods incur huge
compression overhead, thus inefficient in practice.
AQG [81] determines the number of quantization levels according
to the difference between element values obtained by quantization of
the current iteration and that of the last iteration. A precision selection
criterion is proposed to find the most appropriate quantization degree.
The elements with smaller updates can be quantized with a fewer
number of bits when the preset upper limit of bits is not exceeded.
This specific process is to divide the difference into 2ğ‘ âˆ’ 1 intervals
evenly based on the number of bits ğ‘, and find the fixed value of that
interval closest to the original element. AQG designs Augmented AQG
to accommodate client drops. Augmented AQG draws on a gradient
scaling method to ensure the unbiasedness of gradients.
UVeQFed [82] proposes to use subtractive dithered lattice quanti-
zation by assuming that the scalar quantization is less effective than
the vector quantization. This method requires all the workers to use
the same compression operator with the same random seed. It does not
require any prior knowledge about the element distribution. UVeQFed
first initializes a non-singular square matrix as a lattice generator
matrix, which can be considered as a vector version of the Code-Book.
The lattice point is represented by a linear combination of the columns
of the lattice generator matrix. Lattice quantizer maps the original
vector to the closest lattice point to the original value. Since all vectors
nearest to a lattice point are quantized to this point, it is called the
lattice quantization.
The specific quantization process of UVeQFed is as follows. Each
worker scales the model update according to its norm locally. The
scaled model update is split into ğ‘€ copies according to the dimension
of the lattice generator matrix. Then each worker performs random
dithering of these model updates with a shared random seed. Then,
the lattice quantization is used to compress and generate a digital code-
word vector, and this vector is sent to the server which decompresses
the model update by the inverse operation of the above compression
process and performs the aggregation.
5.2.2. Non-uniform quantization
After scaling with the infinite norm or L2-norm of the original
tensor, the normalized gradient distribution is not uniform [83]. Uni-
form quantization cannot well capture the nature of uneven gradient
distribution. Most gradient elements will be mapped to the same quan-
tization level, leading to a large quantization variance. This motivates
the design of non-uniform quantization, given the knowledge of the
gradient distribution. Non-uniform quantization usually falls in the
scope of the Code-Book mapping.
NC (Natural Compression) [84] uses uneven quantization levels to
round the value of an element to one of its two nearest integer powers
of two. It is also an unbiased communication quantization method. The
specific quantization process is shown as below
ğ¶ğ‘ğ¶(ğ‘”ğ‘–) =
â§
âª
â¨
âªâ©
ğ‘ ğ‘–ğ‘”ğ‘›(ğ‘”ğ‘–) â‹…2âŒŠlog2 |ğ‘”ğ‘–|âŒ‹ ğ‘= 2âŒˆlog2 |ğ‘”ğ‘–|âˆ’|ğ‘”ğ‘–|âŒ‰
2âŒŠlog2 |ğ‘”ğ‘–|âŒ‹
ğ‘ ğ‘–ğ‘”ğ‘›(ğ‘”ğ‘–) â‹…2âŒˆlog2 |ğ‘”ğ‘–|âŒ‰ 1 âˆ’ğ‘.
The advantage of NC is that there is no additional computational
overhead other than the randomization process. It is essentially equiv-
alent to removing the mantissa part from the binary expression of gra-
dients and is â€˜â€˜naturallyâ€™â€™ compatible with binary floating-point types.
NUQSGD [83] changes the quantization intervals of QSGD from
a uniform partition between 0 and 1 to a dichotomous partition. If
the total number of quantization levels is ğ‘ + 1, the quantization level
of NUQGSD is
{
0, 1
2ğ‘ , 2
2ğ‘ ,â€¦ ,2ğ‘ âˆ’1
ğ‘  ,1
}
. The main idea of NUQSGD is
to reduce the quantization error and control the variance by dividing
small gradients more finely. The new quantization levels in NUQSGD
is concentrated near zero when the total number of quantization levels
increases. It is not very effective to the quantization of large gradient
elements [85].
APoT [85] is a non-uniform parameter quantization algorithm de-
signed for the bell-shaped and long-tailed parameter distribution. It
aims to solve two challenges. One is how to design quantization levels
efficiently. When designing the non-uniform quantization, there are
still two problems. On the one hand, non-uniform quantization like
NC and NUQSGD overlooked the mapping of dense small weights and
sparse large weights. Higher resolution should be given to the weights
with larger number or more contributions to update in order to make
the model convergence faster. On the other hand, general non-uniform
quantization incurs more compression overhead than uniform quantiza-
tion because they are usually not hardware friendly. Another challenge


--- Page 12 ---
Journal of Systems Architecture 142 (2023) 102927
12
Z. Wang et al.
Table 2
Comparison of different quantization algorithm.
Algorithm Category Uniform or Non-uniform Dynamic quantization
FP16 Truncated expression â€“ Ã—
8-Bit Truncated expression â€“ Ã—
SignSGD Truncated expression â€“ Ã—
QSGD Code-book mapping Uniform Ã—
TernGrad Code-book mapping Uniform Ã—
DIANA Code-book mapping Uniform Ã—
3LC Code-book mapping Uniform Ã—
FedPC Code-book mapping Uniform Ã—
CD-SGD Code-book mapping Uniform Ã—
FedPAQ Code-book mapping Uniform Ã—
LAQ Code-book mapping Uniform Ã—
AdaQS Code-book mapping Uniform âœ“
MQGrad Code-book mapping Uniform âœ“
AQG Code-book mapping Uniform âœ“
UVeQFed Code-book mapping Uniform Ã—
NC Code-book mapping Non-uniform Ã—
NUQSGD Code-book mapping Non-uniform Ã—
APoT Code-book mapping Non-uniform Ã—
ALQ Code-book mapping Non-uniform âœ“
MUSCS Code-book mapping Non-uniform Ã—
is how to choose a suitable threshold for clipping. Before quantization,
clipping is applied to restrict the original weights to a certain range.
However, the range of clipping and the resolution of quantization are
antagonistic. A larger range preserves more weights, but may reduce
the resolution of subsequent quantization. An inappropriate clipping
may cause the network to converge at a slower rate, or even fail
to converge. How to automatically find the appropriate threshold for
clipping in training is still an unresolved problem. APoT proposes
constructing quantization levels based on the sum of basic quadratic
power quantization levels. Only the bit-wise shift operation is in the
quantization process. It is computationally cheap, which only takes
1 clock cycle in modern CPU. This greatly reduces the compression
overhead. The weight normalization is used on the parameters before
quantization to make them more stable and consistent for mapping.
In addition, APoT designs a Reparameterized Clipping Function to
compute a more accurate gradient.
Both ALQ and AMQ [86] are adaptive quantization methods for
adjusting the intervals of different quantization levels. The interval of
quantization levels is determined by minimizing the expected variance
or the normalized expected variance of quantization. ALQ minimizes
the excess variance of quantization for a given gradient distribution
estimate. Contrary to ALQ, AMQ uses exponential quantization levels.
MUCSC [87] uses a clustering method to determine the discrete val-
ues after quantization. It divides original elements into several clusters.
The centroid value of each cluster is considered as the quantization
value of the elements in this cluster. The results of MUCSC are unbiased
estimation of original elements. An enhanced algorithm, namely B-
MUCSC, is proposed to improve the compression degree of MUCSC.
The major difference of B-MUCSC is that it manually defines a super
cluster including elements closest to zero.
5.3. Summary and comparison
In this subsection, we summarize and compare the versatile quanti-
zation algorithms. Gradient quantization aims to reduce the communi-
cation load by lowering the precision of each element. With random
rounding, most quantization methods are unbiased, and the conver-
gence of quantization approaches are guaranteed without the need of
error accumulation compensation. Note that each gradient element is
double-precision (i.e. 32 bits), the compression ratio is no more than
32 times, and at the maximum ratio each elements uses a single bit
to represent its sign. The major differences of existing quantization
approaches are summarized in three aspects in Table 2.
(1) Truncated expression versus Code-book mapping.
One way to quantize a gradient element is its truncated expression
that directly converts the double-precision type to the bit-width of the
certain binary representation. The other is Code-book mapping that
turns a continuous gradient element into one of the predetermined
discrete interval values. Only several bits are needed to represent an
interval and hence the gradient element. The code-book mapping in
general outperforms the truncated expression because the latter is
harmful to the compression of large gradient elements.
(2) Uniform mapping versus Non-uniform mapping.
The spirit of code-book mapping is to represent a gradient element
with less bits. Uniform mapping evenly partitions the range between
the maximum and the minimum values of gradient elements. QSGD
and TernGrad belong to this category. Measurement studies show that
the distribution of gradient elements resembles a bell shape with a long
tail, other than a flat uniform distribution. Hence, the uniform mapping
may introduce large quantization errors, impairing the convergence of
model training. Non-uniform mapping usually configures the intervals
according to the gradient element distribution in order to minimize the
expected quantization errors. NC, NUQSGD, APoT, ALQ and MUCSC
are non-uniform mapping methods that only differ in the specific
techniques used for computing the intervals.
(3) Dynamicity of quantization.
The gradient distribution is dynamically changing as the train-
ing moves forward. An ideal quantization algorithm should be non-
uniform, and keep track of the changing distribution of gradient el-
ements. Methods like QSGD, TernGrad, NC and NUQSGD belong to
static quantization. Their quantization levels or intervals are fixed after
the training starts. The dynamic quantization methods adopt different
approaches to adjust the intervals. AdaQS and MQGrad adaptively
increase or decrease the total number of quantization levels, while AQG
adjusts the precision of the current quantization on the basis of the
previous quantization. The interval of ALQ is determined by solving a
minimization problem of reducing the variance of quantization.
6. Hybrid compression
The essence of sparsification is to reduce the number of elements
and the essence of quantization is to express each element with a fewer
number of bits. These two compression methods are orthogonal in their
core design, implying that they can be combined as hybrid methods.
The effectiveness and feasibility of the hybrid compression has been
proved theoretically in [88].
Sparse one-bit quantization[89] is a hybrid compression method.
Given a threshold ğ‘£, the elements with values between âˆ’ğ‘£ and ğ‘£ are
discarded, while the remaining elements greater than ğ‘£ and less than
