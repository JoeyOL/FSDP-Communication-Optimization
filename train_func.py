import torch
from logger import logger
from tqdm import tqdm
from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
from torch.profiler import record_function
import torch.distributed as dist

from perf.comm_profiler import (
    finalize_monitoring,
    init_monitoring,
    should_stop_early,
    step_begin,
    step_end,
)


# --- æ–°å¢å¸¦ç›‘æ§çš„è®­ç»ƒå‡½æ•° ---
def train_epoch_with_monitoring(model, dataloader, optimizer, scheduler, epoch, rank, world_size, args):
    """è®­ç»ƒä¸€ä¸ªepochï¼Œå¹¶ä½¿ç”¨Profilerå’ŒTensorBoardè¿›è¡Œç›‘æ§"""
    model.train()
    scaler = ShardedGradScaler()
    total_loss = 0.0
    num_batches = len(dataloader)

    # --- TensorBoard å’Œ Profiler è®¾ç½® (ä»…åœ¨ rank 0 ä¸Šæ‰§è¡Œï¼›å®ç°ç»†èŠ‚åœ¨ perf/ ä¸‹) ---
    monitor = init_monitoring(args, rank, num_batches)
    if rank == 0 and monitor.enabled:
        logger.info(f"ğŸ“Š TensorBoard æ—¥å¿—å·²å¯åŠ¨ï¼Œç›®å½•: {monitor.tb_log_dir}")
        logger.info(f"â±ï¸ Profiler å·²å¯åŠ¨ï¼Œè¿½è¸ªæ–‡ä»¶å°†ä¿å­˜è‡³: {monitor.profiler_log_dir}")
    
    dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆåˆå§‹åŒ–

    optimizer.zero_grad()
    
    # ä½¿ç”¨ disable å‚æ•°ï¼Œç¡®ä¿åªæœ‰ rank0 æ‰“å°è¿›åº¦æ¡
    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}", disable=(rank != 0), dynamic_ncols=True)
    
    for batch_idx, batch in enumerate(progress_bar):
        # epoch ä¼ å…¥ä¸º 1-basedï¼Œè¿™é‡Œæ¢ç®—ä¸º 0-based ä»¥ä¿è¯ max_steps è®¡æ•°å‡†ç¡®
        global_step = (epoch - 1) * num_batches + batch_idx
        step_t0 = step_begin(monitor, args)
        try:
            # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
            # é»˜è®¤çº¦å®šï¼šcuda è®¾å¤‡å·²åœ¨å¤–éƒ¨é€šè¿‡ torch.cuda.set_device(local_rank) è®¾ç½®
            batch = {k: v.cuda(non_blocking=True) for k, v in batch.items()}
            
            with record_function("forward_pass"): # Profiler è®°å½•
                outputs = model(**batch)
                loss = outputs.loss
                loss = loss / args.gradient_accumulation_steps
            
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆæŸå¤± (NaN/Inf)ï¼Œè·³è¿‡æ­¤æ­¥ã€‚")
                continue
            
            # 2. åå‘ä¼ æ’­
            with record_function("backward_pass"): # Profiler è®°å½•
                scaler.scale(loss).backward()
            
            total_loss += loss.item() * args.gradient_accumulation_steps

            # 3. æ¢¯åº¦ç´¯ç§¯å’Œæ›´æ–°
            if (batch_idx + 1) % args.gradient_accumulation_steps == 0 or (batch_idx + 1) == num_batches:
                with record_function("optimizer_step"): # Profiler è®°å½•
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                    scheduler.step()

            # --- TensorBoard æ—¥å¿—è®°å½• (ä»…åœ¨rank 0) ---
            if rank == 0 and monitor.tb_writer:
                current_loss = loss.item() * args.gradient_accumulation_steps
                monitor.tb_writer.add_scalar('Loss/step', current_loss, global_step)
                monitor.tb_writer.add_scalar('LearningRate/step', scheduler.get_last_lr()[0], global_step)
                if batch_idx % 20 == 0: # æ¯20æ­¥è®°å½•ä¸€æ¬¡å†…å­˜
                    mem_alloc = torch.cuda.memory_allocated(rank) / 1024**3
                    mem_res = torch.cuda.memory_reserved(rank) / 1024**3
                    monitor.tb_writer.add_scalar('Memory/Allocated_GB', mem_alloc, global_step)
                    monitor.tb_writer.add_scalar('Memory/Reserved_GB', mem_res, global_step)

        except torch.cuda.OutOfMemoryError:
            logger.error(f"æ­¥éª¤ {batch_idx} å‘ç”Ÿ CUDA OOMï¼")
            optimizer.zero_grad()
            torch.cuda.empty_cache()
            continue
        except Exception as e:
            logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            continue

        # è®© profiler schedule å‰è¿› + ï¼ˆå¯é€‰ï¼‰é‡‡é›† step wall time
        step_end(monitor, args, step_t0)

        # --- å¯é€‰ï¼šçŸ­è·‘ï¼Œç”¨äºè€—æ—¶å–è¯ ---
        if should_stop_early(args, global_step):
            if rank == 0:
                logger.info(f"è¾¾åˆ° max_steps={getattr(args, 'max_steps', 0)}ï¼Œæå‰ç»“æŸæœ¬è½®è®­ç»ƒã€‚")
            break
            
        if rank == 0:
            progress_bar.set_postfix({
            'loss': f'{loss.item() * args.gradient_accumulation_steps:.4f}',
            'avg_loss': f'{total_loss/(batch_idx+1):.4f}',
            'lr': f'{scheduler.get_last_lr()[0]:.2e}',
            'gpu_mem': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'
            })

    # --- è®­ç»ƒç»“æŸåæ¸…ç† ---
    if rank == 0:
        finalize_monitoring(monitor, args=args, epoch=epoch, total_loss=total_loss, num_batches=num_batches)
        if monitor.enabled:
            logger.info("â±ï¸ Profiler å·²åœæ­¢ï¼Œå¹¶å·²å†™å‡ºæ‘˜è¦æ–‡ä»¶ã€‚")
            logger.info("ğŸ“Š TensorBoard writer å·²å…³é—­ã€‚")
    
    dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆ

    avg_loss = total_loss / num_batches
    return avg_loss
    

def train_epoch(model, dataloader, optimizer, scheduler, epoch, rank, world_size, args, save_checkpoint_fn=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    scaler = ShardedGradScaler()  # FSDP-compatible GradScaler
    total_loss = 0.0
    num_batches = len(dataloader)
    optimizer.zero_grad()  # åˆå§‹åŒ–æ¢¯åº¦
    
    if rank == 0:
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}")
    else:
        progress_bar = dataloader
    
    for batch_idx, batch in enumerate(dataloader):
        try:
            # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
            batch = {k: v.to(f'cuda:{rank}', non_blocking=True) for k, v in batch.items()}

            # 1. å‰å‘ä¼ æ’­ (åœ¨autocastä¸‹)
            with torch.cuda.amp.autocast(dtype=torch.bfloat16): # æ¨èä½¿ç”¨ bfloat16
                outputs = model(**batch)
                loss = outputs.loss
                # å¯¹ç´¯ç§¯çš„æŸå¤±è¿›è¡Œç¼©æ”¾
                loss = loss / args.gradient_accumulation_steps
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆæŸå¤± (NaN/Inf) åœ¨æ­¥éª¤ {batch_idx}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
                optimizer.zero_grad() # æ¸…ç†æ‰å¯èƒ½å­˜åœ¨çš„åæ¢¯åº¦
                continue
            
            # 2. åå‘ä¼ æ’­ (è®¡ç®—ç¼©æ”¾åçš„æ¢¯åº¦)
            scaler.scale(loss).backward()
            
            total_loss += loss.item() * args.gradient_accumulation_steps # è®°å½•æœªç¼©æ”¾çš„æŸå¤±

            # 3. æ¢¯åº¦ç´¯ç§¯å’Œæ›´æ–°
            if (batch_idx + 1) % args.gradient_accumulation_steps == 0 or (batch_idx + 1) == num_batches:
                # 3.1 (å¯é€‰ä½†æ¨è) æ¢¯åº¦è£å‰ªï¼Œåœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰
                # é¦–å…ˆ unscale æ¢¯åº¦
                scaler.unscale_(optimizer)
                # ç„¶ååœ¨åŸå§‹æ¢¯åº¦ä¸Šè¿›è¡Œè£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                scheduler.step()
                
        except torch.cuda.OutOfMemoryError:
            # æ•è·OOMé”™è¯¯
            logger.error(f"æ­¥éª¤ {batch_idx} å‘ç”Ÿ CUDA Out-of-Memory é”™è¯¯ï¼")
            
            # 1. é‡Šæ”¾ä¸å†éœ€è¦çš„å˜é‡
            # åœ¨Pythonä¸­ï¼Œç¦»å¼€tryå—åï¼Œoutputså’Œlossç­‰å˜é‡ä¼šè‡ªåŠ¨è¢«å›æ”¶ï¼Œ
            # ä½†æ˜¾å¼åˆ é™¤å¯ä»¥æ›´æ¸…æ™°åœ°è¡¨è¾¾æ„å›¾ã€‚
            try:
                del outputs
                del loss
            except NameError:
                # å¦‚æœåœ¨åˆ›å»ºè¿™äº›å˜é‡ä¹‹å‰å°±OOMäº†ï¼Œå®ƒä»¬å¯èƒ½ä¸å­˜åœ¨
                pass

            # 2. æ¸…ç†æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€
            optimizer.zero_grad()

            # 3. å¼ºåˆ¶PyTorché‡Šæ”¾æœªä½¿ç”¨çš„ç¼“å­˜æ˜¾å­˜ (å…³é”®æ­¥éª¤)
            torch.cuda.empty_cache()
            
            logger.warning("å·²é‡Šæ”¾æ˜¾å­˜ç¼“å­˜å¹¶è·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
            continue # ç»§ç»­ä¸‹ä¸€ä¸ªæ‰¹æ¬¡çš„è®­ç»ƒ

        except Exception as e:
            logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            continue
            # æ›´æ–°è¿›åº¦æ¡
        if rank == 0:
            progress_bar.set_postfix({
                'loss': f'{loss.item() * args.gradient_accumulation_steps:.4f}',
                'avg_loss': f'{total_loss/(batch_idx+1):.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.2e}',
                'gpu_mem': f'{torch.cuda.memory_allocated(rank)/1024**3:.1f}GB'
            })
            progress_bar.update(1)
    
    avg_loss = total_loss / num_batches
    return avg_loss