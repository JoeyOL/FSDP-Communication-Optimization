import torch
from logger import logger
from tqdm import tqdm
from pathlib import Path
from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
from torch.profiler import profile, record_function, ProfilerActivity
from torch.utils.tensorboard import SummaryWriter
import torch.distributed as dist


# --- æ–°å¢å¸¦ç›‘æ§çš„è®­ç»ƒå‡½æ•° ---
def train_epoch_with_monitoring(model, dataloader, optimizer, scheduler, epoch, rank, world_size, args):
    """è®­ç»ƒä¸€ä¸ªepochï¼Œå¹¶ä½¿ç”¨Profilerå’ŒTensorBoardè¿›è¡Œç›‘æ§"""
    model.train()
    scaler = ShardedGradScaler()
    total_loss = 0.0
    num_batches = len(dataloader)
    
    # --- TensorBoard å’Œ Profiler è®¾ç½® (ä»…åœ¨rank 0ä¸Šæ‰§è¡Œ) ---
    tb_writer = None
    prof = None
    if rank == 0:
        # å®šä¹‰æ—¥å¿—ç›®å½•
        log_dir = Path(args.output_dir) / "logs" / args.run_name
        tb_log_dir = log_dir / "tensorboard"
        profiler_log_dir = log_dir / "profiler"
        tb_log_dir.mkdir(parents=True, exist_ok=True)
        profiler_log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ– TensorBoard Writer
        tb_writer = SummaryWriter(log_dir=str(tb_log_dir))
        
        # é…ç½® Profiler
        prof = profile(
            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
            on_trace_ready=torch.profiler.tensorboard_trace_handler(str(profiler_log_dir)),
            record_shapes=True,
            profile_memory=True,
            with_stack=True,
            with_flops=True,
            with_modules=True
        )
        prof.start()
        logger.info(f"ğŸ“Š TensorBoard æ—¥å¿—å·²å¯åŠ¨ï¼Œç›®å½•: {tb_log_dir}")
        logger.info(f"â±ï¸ Profiler å·²å¯åŠ¨ï¼Œè¿½è¸ªæ–‡ä»¶å°†ä¿å­˜è‡³: {profiler_log_dir}")
    
    dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆåˆå§‹åŒ–

    optimizer.zero_grad()
    
    # ä½¿ç”¨ disable å‚æ•°ï¼Œæ›´ç®€æ´åœ°æ§åˆ¶è¿›åº¦æ¡åªåœ¨ rank 0 ä¸Šæ˜¾ç¤º
    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}", disable=(rank != 0))
    
    for batch_idx, batch in enumerate(progress_bar):
        global_step = epoch * num_batches + batch_idx
        try:
            # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
            batch = {k: v.to(f'cuda:{rank}', non_blocking=True) for k, v in batch.items()}
            
            with record_function("forward_pass"): # Profiler è®°å½•
                outputs = model(**batch)
                loss = outputs.loss
                loss = loss / args.gradient_accumulation_steps
            
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆæŸå¤± (NaN/Inf)ï¼Œè·³è¿‡æ­¤æ­¥ã€‚")
                continue
            
            # 2. åå‘ä¼ æ’­
            with record_function("backward_pass"): # Profiler è®°å½•
                scaler.scale(loss).backward()
            
            total_loss += loss.item() * args.gradient_accumulation_steps

            # 3. æ¢¯åº¦ç´¯ç§¯å’Œæ›´æ–°
            if (batch_idx + 1) % args.gradient_accumulation_steps == 0 or (batch_idx + 1) == num_batches:
                with record_function("optimizer_step"): # Profiler è®°å½•
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                    scheduler.step()

            # --- TensorBoard æ—¥å¿—è®°å½• (ä»…åœ¨rank 0) ---
            if rank == 0 and tb_writer:
                current_loss = loss.item() * args.gradient_accumulation_steps
                tb_writer.add_scalar('Loss/step', current_loss, global_step)
                tb_writer.add_scalar('LearningRate/step', scheduler.get_last_lr()[0], global_step)
                if batch_idx % 20 == 0: # æ¯20æ­¥è®°å½•ä¸€æ¬¡å†…å­˜
                    mem_alloc = torch.cuda.memory_allocated(rank) / 1024**3
                    mem_res = torch.cuda.memory_reserved(rank) / 1024**3
                    tb_writer.add_scalar('Memory/Allocated_GB', mem_alloc, global_step)
                    tb_writer.add_scalar('Memory/Reserved_GB', mem_res, global_step)

        except torch.cuda.OutOfMemoryError:
            logger.error(f"æ­¥éª¤ {batch_idx} å‘ç”Ÿ CUDA OOMï¼")
            optimizer.zero_grad()
            torch.cuda.empty_cache()
            continue
        except Exception as e:
            logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            continue
            
        if rank == 0:
            progress_bar.set_postfix({
            'loss': f'{loss.item() * args.gradient_accumulation_steps:.4f}',
            'avg_loss': f'{total_loss/(batch_idx+1):.4f}',
            'lr': f'{scheduler.get_last_lr()[0]:.2e}',
            'gpu_mem': f'{torch.cuda.memory_allocated(rank)/1024**3:.1f}GB'
            })

    # --- è®­ç»ƒç»“æŸåæ¸…ç† ---
    if rank == 0:
        if prof:
            prof.stop()
            logger.info("â±ï¸ Profiler å·²åœæ­¢ã€‚")
        if tb_writer:
            avg_epoch_loss = total_loss / num_batches
            tb_writer.add_scalar('Loss/epoch', avg_epoch_loss, epoch)
            tb_writer.close()
            logger.info("ğŸ“Š TensorBoard writer å·²å…³é—­ã€‚")
    
    dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆ

    avg_loss = total_loss / num_batches
    return avg_loss
    

def train_epoch(model, dataloader, optimizer, scheduler, epoch, rank, world_size, args, save_checkpoint_fn=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    scaler = ShardedGradScaler()  # FSDP-compatible GradScaler
    total_loss = 0.0
    num_batches = len(dataloader)
    optimizer.zero_grad()  # åˆå§‹åŒ–æ¢¯åº¦
    
    if rank == 0:
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}")
    else:
        progress_bar = dataloader
    
    for batch_idx, batch in enumerate(dataloader):
        try:
            # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
            batch = {k: v.to(f'cuda:{rank}', non_blocking=True) for k, v in batch.items()}

            # 1. å‰å‘ä¼ æ’­ (åœ¨autocastä¸‹)
            with torch.cuda.amp.autocast(dtype=torch.bfloat16): # æ¨èä½¿ç”¨ bfloat16
                outputs = model(**batch)
                loss = outputs.loss
                # å¯¹ç´¯ç§¯çš„æŸå¤±è¿›è¡Œç¼©æ”¾
                loss = loss / args.gradient_accumulation_steps
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆæŸå¤± (NaN/Inf) åœ¨æ­¥éª¤ {batch_idx}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
                optimizer.zero_grad() # æ¸…ç†æ‰å¯èƒ½å­˜åœ¨çš„åæ¢¯åº¦
                continue
            
            # 2. åå‘ä¼ æ’­ (è®¡ç®—ç¼©æ”¾åçš„æ¢¯åº¦)
            scaler.scale(loss).backward()
            
            total_loss += loss.item() * args.gradient_accumulation_steps # è®°å½•æœªç¼©æ”¾çš„æŸå¤±

            # 3. æ¢¯åº¦ç´¯ç§¯å’Œæ›´æ–°
            if (batch_idx + 1) % args.gradient_accumulation_steps == 0 or (batch_idx + 1) == num_batches:
                # 3.1 (å¯é€‰ä½†æ¨è) æ¢¯åº¦è£å‰ªï¼Œåœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰
                # é¦–å…ˆ unscale æ¢¯åº¦
                scaler.unscale_(optimizer)
                # ç„¶ååœ¨åŸå§‹æ¢¯åº¦ä¸Šè¿›è¡Œè£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                scheduler.step()
                
        except torch.cuda.OutOfMemoryError:
            # æ•è·OOMé”™è¯¯
            logger.error(f"æ­¥éª¤ {batch_idx} å‘ç”Ÿ CUDA Out-of-Memory é”™è¯¯ï¼")
            
            # 1. é‡Šæ”¾ä¸å†éœ€è¦çš„å˜é‡
            # åœ¨Pythonä¸­ï¼Œç¦»å¼€tryå—åï¼Œoutputså’Œlossç­‰å˜é‡ä¼šè‡ªåŠ¨è¢«å›æ”¶ï¼Œ
            # ä½†æ˜¾å¼åˆ é™¤å¯ä»¥æ›´æ¸…æ™°åœ°è¡¨è¾¾æ„å›¾ã€‚
            try:
                del outputs
                del loss
            except NameError:
                # å¦‚æœåœ¨åˆ›å»ºè¿™äº›å˜é‡ä¹‹å‰å°±OOMäº†ï¼Œå®ƒä»¬å¯èƒ½ä¸å­˜åœ¨
                pass

            # 2. æ¸…ç†æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€
            optimizer.zero_grad()

            # 3. å¼ºåˆ¶PyTorché‡Šæ”¾æœªä½¿ç”¨çš„ç¼“å­˜æ˜¾å­˜ (å…³é”®æ­¥éª¤)
            torch.cuda.empty_cache()
            
            logger.warning("å·²é‡Šæ”¾æ˜¾å­˜ç¼“å­˜å¹¶è·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
            continue # ç»§ç»­ä¸‹ä¸€ä¸ªæ‰¹æ¬¡çš„è®­ç»ƒ

        except Exception as e:
            logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            continue
            # æ›´æ–°è¿›åº¦æ¡
        if rank == 0:
            progress_bar.set_postfix({
                'loss': f'{loss.item() * args.gradient_accumulation_steps:.4f}',
                'avg_loss': f'{total_loss/(batch_idx+1):.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.2e}',
                'gpu_mem': f'{torch.cuda.memory_allocated(rank)/1024**3:.1f}GB'
            })
            progress_bar.update(1)
    
    avg_loss = total_loss / num_batches
    return avg_loss